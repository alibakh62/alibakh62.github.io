<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>random_forest.md</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
<style>
/* Style the button that is used to open and close the collapsible content */
.collapsible {
  background-color: #eee;
  color: #444;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.active, .collapsible:hover {
  background-color: #ccc;
}

/* Style the collapsible content. Note: hidden by default */
.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
  background-color: #f1f1f1;
}
</style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#building-and-evaluating-random-forest">Building and evaluating random forest</a>
<ul>
<li><a href="#step-1">Step 1</a></li>
<li><a href="#step-2">Step 2</a></li>
<li><a href="#now-go-back-to-step-1-and-repeat">Now go back to step 1 and repeat</a></li>
<li><a href="#estimate-accuracy-how-do-we-know-if-its-any-good">Estimate accuracy: How do we know if it’s any good?</a></li>
<li><a href="#choosing-the-number-of-subset-variables">Choosing the number of subset variables</a></li>
</ul>
</li>
<li><a href="#missing-data-and-sample-clustering">Missing data and sample clustering</a>
<ul>
<li><a href="#missing-data-in-the-original-dataset">Missing data in the original dataset</a></li>
<li><a href="#missing-data-in-a-new-sample">Missing data in a new sample</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <p><a href="https://youtube.com/playlist?list=PLblh5JKOoLUIE96dI3U7oxHaCAbZgfhHk">Source</a></p>
<h1 id="building-and-evaluating-random-forest">Building and evaluating random forest</h1>
<ul>
<li>Random forest is made out of decision trees.</li>
<li>Decision trees are easy to build and interpret.
<ul>
<li>But, in practice, they’re not that good.</li>
<li>Trees have one aspect that prevents them from being the ideal tool for predictive learning, namely <em><strong>inaccuracy</strong></em>.</li>
<li>In other words, they’re great with the data used to create them, but they’re not flexible when classifying new samples.</li>
</ul>
</li>
<li>Random forest combines the simplicity of decision trees with flexibility resulting in a vast improvement in accuracy.</li>
</ul>
<h2 id="step-1">Step 1</h2>
<p>Create a “bootstrapped” dataset.</p>
<ul>
<li>To create a bootstrapped dataset that is the same size as the original, we just randomly select samples from the original dataset.</li>
<li>The <strong>important detail</strong> is that we’re allowed to pick the same sample more than once.</li>
</ul>
<h2 id="step-2">Step 2</h2>
<p>Create a decision tree using the bootstrapped dataset, but only use a random subset of variables (or columns) at each step.</p>
<ul>
<li>
<p><strong>Note:</strong> We’ll discuss how to find the optimal number of variables to consider a little later.</p>
</li>
<li>
<p>We first pick, say 3, variables and find the best one to be the <em><strong>root node</strong></em>.</p>
</li>
<li>
<p>After selecting the root node variable, we leave that out, and pick another 3 from the rest of the variables, and we basically build the tree like that.</p>
</li>
<li>
<p><strong>Note</strong> that in each step, we only use a subset of variables instead of all.</p>
</li>
</ul>
<h2 id="now-go-back-to-step-1-and-repeat">Now go back to step 1 and repeat</h2>
<p>Make a new bootstrapped dataset and build a tree considering a subset of variables at each step.</p>
<ul>
<li>Ideally, you’d do this 100’s of times.</li>
<li>Using a bootstrapped sample and considering only a subset of the variables at each step results in a wide variety of trees.</li>
<li>The <em><strong>variety</strong></em> is what makes the random forest more effective than individual trees.</li>
</ul>
<p><strong>Now that we’ve created a random forest, how do we use it?</strong></p>
<ul>
<li>We run each row of the data down all of the trees in the random forest, we see which option received more votes.</li>
<li><strong>B</strong>ootstrapping the data plus using the <strong>agg</strong>regate to make a decision is called “<strong>Bagging</strong>”.</li>
</ul>
<h2 id="estimate-accuracy-how-do-we-know-if-its-any-good">Estimate accuracy: How do we know if it’s any good?</h2>
<ul>
<li>When we created the bootstrapped dataset, we allowed duplicate entries. As a result, some entries may not get included in the bootstrapped dataset.</li>
<li>Typically, about <strong>1/3</strong> of the original data does not end up in the bootstrapped dataset. This is called the <strong>“Out-of-Bag Dataset”</strong>.</li>
<li>We run all the out-of-bag data through all the trees and find the votes for them.</li>
<li>Ultimately, we can measure how accurate our random forest is by the proportion of out-of-bag samples that were correctly classified by the random forest.</li>
<li>The proportion of out-of-bag samples that were <em>incorrectly</em> classified is the <strong>“out-of-bag error”</strong>.</li>
</ul>
<h2 id="choosing-the-number-of-subset-variables">Choosing the number of subset variables</h2>
<ul>
<li>Now that we know how to measure the accuracy of a random forest, we can build random forests with the different number of variables subsets and choose the number that yields the highest accuracy.</li>
<li>Typically, we start by using the square of the number of variables and then try a few settings above and below that value.</li>
</ul>
<h1 id="missing-data-and-sample-clustering">Missing data and sample clustering</h1>
<p>Random forest consider two types of missing data:</p>
<ol>
<li>Missing data in the original dataset used to create the random forest.</li>
<li>Missing data in a new sample that you want to categorize.</li>
</ol>
<h2 id="missing-data-in-the-original-dataset">Missing data in the original dataset</h2>
<p>The general idea for dealing with missing data in this context is to make an initial guess that could be bad, then gradually refine the guess until it is (hopefully) a good guess.</p>
<h3 id="the-initial-guess">The initial guess</h3>
<p>Based on the label for that row of data that has missing values, we make initial guesses for the columns with missing values based on the most common value (categorical) or median (for continuous) of the non-missing data with the same label.</p>
<center><a href="https://www.flickr.com/photos/192167571@N04/51740543533/in/dateposted-friend/" title="Screen Shot 2021-12-11 at 5.54.54 PM"><img src="https://live.staticflickr.com/65535/51740543533_eafd45b748_c.jpg" width="50%" height="50%" alt="Screen Shot 2021-12-11 at 5.54.54 PM"></a></center>
<h3 id="refine-initial-guess">Refine initial guess</h3>
<p>We do this by first determining which samples are similar to the ones with missing data.</p>
<p><strong>How to determine similarity?</strong></p>
<ol>
<li>Build a random forest.</li>
<li>Run all of the data down all of the trees</li>
<li>For each tree:
<ul>
<li><em><strong>If two samples (rows of data) end up in the same leaf node, they’re similar.</strong></em></li>
<li>For each tree, we find similar samples and add +1 to samples that are similar.</li>
<li>Ultimately, we run the data down all the trees and the proximity matrix fills in.</li>
</ul>
</li>
<li>Then, we divide each proximity value by the total number of trees.</li>
<li>Now, we replace the missing values using the proximity values.
<ul>
<li>Here, proximity numbers are used as weights to calculate the missing value.</li>
<li><strong>Note:</strong> we’ll normalize the value by diving it by the sum of all proximity values for that sample. (see examples below for categorical and numeric values)</li>
<li><strong>Note:</strong> for categorical values we calculate the weight for each value and choose the one with the highest weighted frequency (see image below).</li>
<li><strong>Note:</strong> For numeric values, it’s just the weighted sum of non-missing values where the weights are proximity values (see below).</li>
</ul>
</li>
<li>We do the whole thing over again (i.e. 1. Build a random forest, 2. Run the data through the trees, 3. Recalculate the proximity and missing values), until the (guessed) missing values converge.</li>
</ol>
<center><a href="https://www.flickr.com/photos/192167571@N04/51745483880/in/dateposted-friend/" title="Screen Shot 2021-12-13 at 12.35.29 PM"><img src="https://live.staticflickr.com/65535/51745483880_c380663875_z.jpg" width="640" height="325" alt="Screen Shot 2021-12-13 at 12.35.29 PM"></a></center>
<center><a href="https://www.flickr.com/photos/192167571@N04/51745240524/in/dateposted-friend/" title="Screen Shot 2021-12-13 at 12.33.03 PM"><img src="https://live.staticflickr.com/65535/51745240524_35db62a521_z.jpg" width="640" height="323" alt="Screen Shot 2021-12-13 at 12.33.03 PM"></a></center>
<center><a href="https://www.flickr.com/photos/192167571@N04/51745483880/in/dateposted-friend/" title="Screen Shot 2021-12-13 at 12.35.29 PM"><img src="https://live.staticflickr.com/65535/51745483880_c380663875_z.jpg" width="640" height="325" alt="Screen Shot 2021-12-13 at 12.35.29 PM"></a></center>
<center><a href="https://www.flickr.com/photos/192167571@N04/51744841358/in/dateposted-friend/" title="Screen Shot 2021-12-13 at 1.17.20 PM"><img src="https://live.staticflickr.com/65535/51744841358_bac1067954_z.jpg" width="640" height="326" alt="Screen Shot 2021-12-13 at 1.17.20 PM"></a></center>
<h3 id="distance-matrix">Distance matrix</h3>
<p>We can do something cool with the proximity matrix. When two samples end up in the same leaf node for all the trees, their (normalized) proximity score is 1, which means the samples are as close as can be. That means <span class="katex--inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mtext>proximity</mtext><mo>=</mo><mtext>distance</mtext></mrow><annotation encoding="application/x-tex">1 - \text{proximity} = \text{distance}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.72777em; vertical-align: -0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em;"></span></span><span class="base"><span class="strut" style="height: 0.8623em; vertical-align: -0.19444em;"></span><span class="mord text"><span class="mord">proximity</span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span></span><span class="base"><span class="strut" style="height: 0.69444em; vertical-align: 0em;"></span><span class="mord text"><span class="mord">distance</span></span></span></span></span></span> (i.e. close as can be = no distance between). We can draw a <strong>heatmap</strong> based on a <strong>distance matrix</strong>. We can also draw an <strong>MDS plot</strong>.</p>
<p>The cool thing about this is that no matter what the data are (ranks, multiple-choice, numeric, etc.), if we can use it to make a tree, we can draw a heatmap (or an MDS plot) to show how the samples are related to each other.</p>
<center><a href="https://www.flickr.com/photos/192167571@N04/51745426394/in/dateposted-friend/" title="Screen Shot 2021-12-13 at 2.53.14 PM"><img src="https://live.staticflickr.com/65535/51745426394_18e15dfb7a_z.jpg" width="640" height="251" alt="Screen Shot 2021-12-13 at 2.53.14 PM"></a></center>
<h2 id="missing-data-in-a-new-sample">Missing data in a new sample</h2>
<p>In this case, since we don’t have the labels, we have to do this:</p>
<ul>
<li>The first thing is to create two (imagine binary categories) copies of the data with each of the labels.</li>
<li>Then, we use the iterative method from above to make a good guess about the missing values.</li>
<li>Once we filled out the missing values, we run the two samples down the random forests to see which of the two are correctly labeled by the random forest the most times.
<ul>
<li>The one with more correctly classified instances wins.</li>
</ul>
</li>
</ul>

    </div>
  </div>
  <script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
<script src="https://gist.github.com/username/a39a422ebdff6e732753b90573100b16.js"></script>
</body>

</html>
