<!DOCTYPE html><html><head>
      <title>3_probabilistic_path_planning</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:////Users/abakh005/.vscode/extensions/shd101wyy.markdown-preview-enhanced-0.6.3/node_modules/@shd101wyy/mume/dependencies/katex/katex.min.css">
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <h1 class="mume-header" id="introduction-to-sample-based-probabilistic-path-planning">Introduction to Sample-Based &amp; Probabilistic Path Planning</h1>

<p>The examples that we investigated in discrete (or combinatorial) planning were quite simple. They were two-dimensional examples of limited size. There are times when you can simplify your environment and robot to a two-dimensional representation. For instance, a vaccum robot traversing somebody&apos;s house. In such as case, there isn&apos;t a need for a complex three-dimensional representation, as a little robot can&apos;t do anything more than translate and rotate on the 2D plane that is the floor.</p>
<p>But, there will also be times when you find yourself limited by the two-dimensional representation and will need to work in a three-dimensional world, with robots that have six degrees of freedom or more. Such a scenario is a lot more challenging to perform path planning in. It may be possible to apply the algorithms that you learned in the previous lesson, however, the efficiency of these algorithms becomes more and more critical.</p>
<p>Performing a complete discretization of the entire space and applying a graph search algorithm to the space may be too costly. To tackle the path planning problems of larger size and greater dimension, there exists alternate algorithms that fall under the umbrella of the <strong>sample-based path planning</strong>.</p>
<p>Instead of conducting a complete discretization of the configuration space, there algorithms randomly sample the space hoping that the collection of samples will adequately represent the configuration space.</p>
<p>See the video <a href="https://youtu.be/msgVwQCM2C8">here</a>.</p>
<h1 class="mume-header" id="why-sample-based-planning">Why Sample-Based Planning?</h1>

<p>So why exactly can&#x2019;t we use discrete planning for higher dimensional problems? Well, it&#x2019;s incredibly hard to discretize such a large space. The complexity of the path planning problem increases exponentially with the number of dimensions in the C-space.</p>
<h3 class="mume-header" id="increased-dimensionality">Increased Dimensionality</h3>

<p>For a 2-dimensional 8-connected space, every node has 8 successors (8-connected means that from every cell you can move laterally or diagonally). Imagine a 3-dimensional 8-connected space, how many successors would every node have? 26. As the dimension of the C-space grows, the number of successors that every cell has increases substantially. In fact, for an n-dimensional space, it is equal to <code>3^n - 1</code>.</p>
<p>It is not uncommon for robots and robotic systems to have large numbers of dimensions. Recall the robotic arm that you worked with in the pick-and-place project - that was a 6-DOF arm. If multiple 6-DOF arms work in a common space, the computation required to perform path planning to avoid collisions increases substantially. Then, think about the complexity of planning for humanoid robots such as the one depicted below. Such problems may take intolerably long to solve using the combinatorial approach.</p>
<p align="center">
<img src="img/sample-based-planning1.png" alt="drawing" width="600">
</p>
<h3 class="mume-header" id="constrained-dynamics">Constrained Dynamics</h3>

<p>Aside from robots with many degrees of freedom and multi-robot systems, another computational difficulty involves working with robots that have constrained dynamics. For instance, a car is limited in its motion - it can move forward and backward, and it can turn with a limited turning radius - as you can see in the image below.</p>
<p align="center">
<img src="img/sample-based-planning2.png" alt="drawing" width="600">
</p>
<p>However, the car is not able to move laterally - as depicted in the following image. (As unfortunate as it is for those of us that struggle to parallel park!)</p>
<p align="center">
<img src="img/sample-based-planning3.png" alt="drawing" width="600">
</p>
<p>In the case of the car, more complex motion dynamics must be considered when path planning - including the derivatives of the state variables such as velocity. For example, a car&apos;s safe turning radius is dependent on it&apos;s velocity.</p>
<p>Robotic systems can be classified into two different categories - holonomic and non-holonomic. <strong>Holonomic systems</strong> can be defined as systems where every constraint depends exclusively on the current pose and time, and not on any derivatives with respect to time. <strong>Nonholonomic systems</strong>, on the other hand, are dependent on derivatives. Path planning for nonholonomic systems is more difficult due to the added constraints.</p>
<p>In this section, you will learn two different path planning algorithms, and understand how to tune their parameters for varying applications.</p>
<h1 class="mume-header" id="weakening-requirements">Weakening Requirements</h1>

<p>Combinatorial path planning algorithms are too inefficient to apply in high-dimensional environments, which means that some practical compromise is required to solve the problem! Instead of looking for a path planning algorithm that is both complete and optimal, what if the requirements of the algorithm were weakened?</p>
<p>Instead of aspiring to use an algorithm that is complete, the requirement can be weakened to use an algorithm that is probabilistically complete. A <strong>probabilistically complete</strong> algorithm is one who&#x2019;s probability of finding a path, if one exists, increases to 1 as time goes to infinity.</p>
<p>Similarly, the requirement of an optimal path can be weakened to that of a feasible path. A <strong>feasible path</strong> is one that obeys all environmental and robot constraints such as obstacles and motion constraints. For high-dimensional problems with long computational times, it may take unacceptably long to find the optimal path, whereas a feasible path can be found with relative ease. Finding a feasible path proves that a path from start to goal exists, and if needed, the path can be optimized locally to improve performance.</p>
<p>Sample-based planning is probabilistically complete and looks for a feasible path instead of the optimal path.</p>
<h1 class="mume-header" id="sample-based-path-planning">Sample-Based Path Planning</h1>

<p>Sample-based path planning differs from combinatorial path planning in that it does not try to systematically discretize the entire configuration space. Instead, it samples the configuration space randomly (or semi-randomly) to build up a representation of the space. The resultant graph is not as precise as one created using combinatorial planning, but it is much quicker to construct because of the relatively small number of samples used.</p>
<p>Such a method is probabilistically complete because as time passes and the number of samples approaches infinity, the probability of finding a path, if one exists, approaches 1.</p>
<p>Such an approach is very effective in high-dimensional spaces, however it does have some downfalls. Sampling a space uniformly is not likely to reach small or narrow areas, such as the passage depicted in the image below. Since the passage is the only way to move from start to goal, it is critical that a sufficient number of samples occupy the passage, or the algorithm will return &#x2018;no solution found&#x2019; to a problem that clearly has a solution.</p>
<p align="center">
<img src="img/sample-based-planning4.png" alt="drawing" width="600">
</p>
<p>Different sample-based planning approaches exist, each with their own benefits and downfalls. In the next few pages you will learn about,</p>
<ul>
<li>Probabilistic Roadmap Method</li>
<li>Rapidly Exploring Random Tree Method</li>
</ul>
<p>You will also learn about Path Smoothing - one improvement that can make resultant paths more efficient.</p>
<h1 class="mume-header" id="probabilistic-roadmap-prm">Probabilistic Roadmap (PRM)</h1>

<p>One common sampled-based path planning method is called the <strong>Probabilistic Roadmap (PRM)</strong>. PRM randomly samples the workspace, building up a graph to represent the free space.</p>
<p>It does so without needing to construct the C space or discretize it. All that PRM requires is a collision check function to test whether a randomly generated nodes lies in the free space or is in collision with an obstacle.</p>
<p>Let&apos;s consider the below workspace and add a few random samples to this workspace and see how PRM works.</p>
<p align="center">
<img src="img/prm1.png" alt="drawing" width="600">
</p>
<p>The process of building up a graph is called the <strong>learning phase</strong>. Since that&apos;s what the PRM does by sampling random configurations and adding them to the graph. It does so by generating a new random configuration represented by a node in the graph, and checking to see if it is in collision. If it is not, like the node you see here (the purple one), then PRM will try to connect the node to its neighbors. There are few different ways of doing so.</p>
<ul>
<li>PRM can look for any number of neighbors within a certain radius of the node, or</li>
<li>it could look for the nodes K nearest neighbors.</li>
</ul>
<p>Once the neighbors have been selected, PRM will see if they can successfully create an edge to each of its neighbors. As you see in the image above, one edge is in collision with an obstacle while the other three are safe to add. This node has been added to the graph and now this process can be repeated for another randomly generated node. The local planner must find a path between tow nodes or return that such a path does not exist, and it must do so quickly, since this step is repeated for every neighbor of every new node.</p>
<p>One easy way to accomplish this is to draw a straight line between the two nodes and then check if any part of it collides with an obstacle. To do this, you can place a number of evenly spaced samples on the line (image below) and see whether any one of them is in a collision. You can work incrementally starting on one side of the edge and moving towards the other, or you can take a binary approach, checking the sample at the midpoint first. In this image below, that returns a collision right away. But if you didn&apos;t, you could continue breaking the edge up into segments, checking each midpoint sample for a collision. If all of the samples return no collision found, then the edge can be added to the graph.</p>
<p align="center">
<img src="img/prm2.png" alt="drawing" width="600">
</p>
<p>The process of adding new nodes and connecting them to the graph continues. Eventually, a certain criteria is met, such as a specific number of nodes or edges have been created, or a particular amount of time has elapsed. <strong>At this point, the learning phase is over</strong>. Then, PRM enters the <strong>query phase</strong>, where it uses the resulting graph to find a path from start to goal.</p>
<ul>
<li>First, it must connect each of these to the graph. PRM does so by looking for the nodes closest to the start and goal and using the local planner to try to build a connection.</li>
<li>If this process is successful, then a search algorithm like A* search can be applied to find the path from start to goal. The resulting path may not be optimal, but it proves that moving from start to goal is feasible.</li>
</ul>
<p>See the video <a href="https://youtu.be/hs9Xkujs-5M">here</a>.</p>
<h3 class="mume-header" id="algorithm">Algorithm</h3>

<p>The pseudocode for the PRM learning phase is provided below.</p>
<p align="center">
<img src="img/prm-alg.png" alt="drawing" width="600">
</p>
<p>After the learning phase, comes the query phase.</p>
<h3 class="mume-header" id="setting-parameters">Setting Parameters</h3>

<p>There are several parameters in the PRM algorithm that require tweaking to achieve success in a particular application. Firstly, the <strong>number of iterations</strong> can be adjusted - the parameter controls between how detailed the resultant graph is and how long the computation takes. For path planning problems in wide-open spaces, additional detail is unlikely to significantly improve the resultant path. However, the additional computation is required in complicated environments with narrow passages between obstacles. Beware, setting an insufficient number of iterations can result in a &#x2018;path not found&#x2019; if the samples do not adequately represent the space.</p>
<p>Another decision that a robotics engineer would need to make is <strong>how to find neighbors</strong> for a randomly generated configuration. One option is to look for the k-nearest neighbors to a node. To do so efficiently, a <a href="https://xlinux.nist.gov/dads/HTML/kdtree.html">k-d</a> tree can be utilized - to break up the space into &#x2018;bins&#x2019; with nodes, and then search the bins for the nearest nodes. Another option is to search for any nodes within a certain distance of the goal. Ultimately, knowledge of the environment and the solution requirements will drive this decision-making process.</p>
<p>The choice for what type of <strong>local planner</strong> to use is another decision that needs to be made by the robotics engineer. The local planner demonstrated in the video is an example of a very simple planner. For most scenarios, a simple planner is preferred, as the process of checking an edge for collisions is repeated many times (k*n times, to be exact) and efficiency is key. However, more powerful planners may be required in certain problems. In such a case, the local planner could even be another PRM.</p>
<h3 class="mume-header" id="probabilistically-complete">Probabilistically Complete</h3>

<p>As discussed before, sample-based path planning algorithms are probabilistically complete. Now that you have seen one such algorithm in action, you can see why this is the case. As the number of iterations approaches infinity, the graph approaches completeness and the optimal path through the graph approaches the optimal path in reality.</p>
<h3 class="mume-header" id="variants">Variants</h3>

<p>The algorithm that you learned here is the vanilla version of PRM, but many other variations to it exist. The following link discusses several alternative strategies for implementing a PRM that may produce a more optimal path in a more efficient manner.</p>
<ul>
<li><a href="http://www.staff.science.uu.nl/~gerae101/pdf/compare.pdf">A Comparative Study of Probabilistic Roadmap Planners</a></li>
</ul>
<h3 class="mume-header" id="prm-is-a-multi-query-planner">PRM is a Multi-Query Planner</h3>

<p>The Learning Phase takes significantly longer to implement than the Query Phase, which only has to connect the start and goal nodes, and then search for a path. However, the graph created by the Learning Phase can be reused for many subsequent queries. For this reason, PRM is called a <strong>multi-query planner</strong>.</p>
<p>This is very beneficial in static or mildly-changing environments. However, some environments change so quickly that PRM&#x2019;s multi-query property cannot be exploited. In such situations, PRM&#x2019;s additional detail and computational slow nature is not appreciated. A quicker algorithm would be preferred - one that doesn&#x2019;t spend time going in all directions without influence by the start and goal.</p>
<p><strong>Some Notes on Probabilistic Roadmaps</strong></p>
<ul>
<li>PRM is indeed a multi-query planner. In it&#x2019;s Learning Phase, PRM generates a representation of the entire environment, which can be applied in future queries.</li>
<li>With an insufficient number of configurations in the graph, or many configurations resulting in collisions, a path may not be constructed between start and goal even if one exists.</li>
<li>Due to the random sampling of nodes, the true optimal path is unlikely to be present in the connected graph.</li>
</ul>
<h1 class="mume-header" id="rapidly-exploring-random-tree-method-rrt">Rapidly Exploring Random Tree Method (RRT)</h1>

<p>Another commonly utilized sample-based planning method is the <strong>rapidly exploring random tree</strong> method. RRT differs from PRM in that it is a single query planner. If you recall, PRM spent its learning phase building up a representation of the entire workspace. This was <strong>computationally expensive</strong>, but the resultant graph can be used for multiple queries.</p>
<p>RRT disregards the need for a comprehensive graph, and builds on a new for each individual query, taking into account the start and goal positions as it does so. This results in a much smaller but more directed graph with a faster computation time.</p>
<p>PRM is great for static environments, where you can reuse the graph, but certain environments change too quickly and the RRT method serves these environments well.</p>
<p>Let&apos;s see what the RRT method looks like in the same environment as before with the same start and goal configurations. However, instead of adding these in the learning phase, they will be explicitly considered from the start.</p>
<p>Then, we start to build up a representation of the workspace. <strong>While the PRM built up a graph, RRT will build a tree, that is a type of graph where each node only has one parent</strong>.</p>
<p>In a <strong>single query planner</strong>, you are only concerned about getting from start to goal and the lack of lateral connections between seemingly neighboring nodes is less of a concern. So, <strong>what&apos;s the algorithm?</strong></p>
<ul>
<li>RRT will randomly generate a node; then it will find its closest neighbor.</li>
<li>If the node is within a certain distance, <strong><code>&#x3B4;</code></strong> of the neighbor, then it can be connected directly, if the local planner determines the edge to be collision-free.</li>
<li>However, if a newly generated node is a far distance away from all other nodes, then the chance of the edge between the node and and its nearest neighbor being collision-free is unlikely. In such a case, instead of connecting to this node, RRT will create a new node in the same direction, but a distance <strong><code>&#x3B4;</code></strong> away. Then, this edge is checked for collisions, and if it&apos;s in the clear, the node is added to the tree.</li>
<li>Nodes can be generated by uniformly sampling the search space, which would favor wide unexplored spaces, or alternatively, some greediness can be introduced by increasing the probability of sampling near the goal, which would bias new samples in the direction of the goal.</li>
<li>Since RRT is a single query planner, slight biasing is often favorable to introduce.</li>
<li>One variation of the RRT method is one that grows two trees: one from the start and one from the goal. RRT alternates growing each tree, and at every step it tries to build an edge between the most recently added node and the other tree. Eventually, it succeeds and RRT knows that a path has been found from start to goal.</li>
</ul>
<p><strong>NOTE:</strong> PRM and RRT are exemplary performers in multidimensional spaces for robots with many degrees of freedom. In fact, they have been able to solve problems that traditional path planning algorithms are unable to solve.</p>
<p>See the video <a href="https://youtu.be/kOS76UR7Fo8">here</a>.</p>
<h3 class="mume-header" id="algorithm-1">Algorithm</h3>

<p>The pseudocode for the RRT learning phase is provided below.</p>
<p align="center">
<img src="img/rrt-alg.png" alt="drawing" width="600">
</p>
<h3 class="mume-header" id="setting-parameters-1">Setting Parameters</h3>

<p>Just like with PRM, there are a few parameters that can be tuned to make RRT more efficient for a given application.</p>
<p>The first of these parameters is the sampling method (ie. how a random configuration is generated). As discussed in the video, you can sample uniformly - which would favour wide unexplored spaces, or you can sample with a bias - which would cause the search to advance greedily in the direction of the goal. Greediness can be beneficial in simple planning problems, however in some environments it can cause the robot to get stuck in a local minima. It is common to utilize a uniform sampling method with a small hint of bias.</p>
<p>The next parameter that can be tuned is <code>&#x3B4;</code>. As RRT starts to generate random configurations, a large proportion of these configurations will lie further than a distance <code>&#x3B4;</code> from the closest configuration in the graph. In such a situation, a randomly generated node will dictate the direction of growth, while <code>&#x3B4;</code> is the growth rate.</p>
<p>Choosing a small <code>&#x3B4;</code> will result in a large density of nodes and small growth rate. On the other hand, choosing a large <code>&#x3B4;</code> may result in lost detail, as well as an increasing number of nodes being unable to connect to the graph due to the greater chance of collisions with obstacles. <code>&#x3B4;</code> must be chosen carefully, with knowledge of the environment and requirements of the solution.</p>
<h3 class="mume-header" id="single-query-planner">Single-Query Planner</h3>

<p>Since the RRT method explores the graph starting with the start and goal nodes, the resultant graph cannot be applied to solve additional queries. RRT is a single-query planner.</p>
<p>RRT is, however, much quicker than PRM at solving a path planning problem. This is so because it takes into account the start and end nodes, and limits growth to the area surrounding the existing graph instead of reaching out into all distant corners, the way PRM does. RRT is more efficient than PRM at solving large path planning problems (ex. ones with hundreds of dimensions) in dynamic environments.</p>
<p>Generally speaking, RRT is able to solve problems with 7 dimensions in a matter of milliseconds, and may take several minutes to solve problems with over 20 dimensions. In comparison, such problems would be impossible to solve with the combinatorial path planning method.</p>
<h3 class="mume-header" id="rrt-non-holonomic-systems">RRT &amp; Non-holonomic Systems</h3>

<p>While we will not go into significant detail on this topic, the RRT method supports planning for non-holonomic systems, while the PRM method does not. This is so because the RRT method can take into consideration the additional constraints (such as a car&#x2019;s turning radius at a particular speed) when adding nodes to a graph, the same way it already takes into consideration how far away a new node is from an existing tree.</p>
<p><strong>Some Notes on RRT:</strong></p>
<ul>
<li>When a randomly generated configuration is greater than a distance, delta, from its closest neighbour, the algorithm will create a new node precisely a distance delta away, but along the same path.</li>
<li>Unlike PRM, RRT is able to process the additional constraints to handle non-holonomic systems.</li>
<li>If delta is set to a small value, the growth rate will be limited. If delta is set to a large value, it is more likely that edges between new and existing nodes will be in collision with obstacles.</li>
</ul>
<h1 class="mume-header" id="path-smoothing">Path Smoothing</h1>

<p>When you look at the paths produced by PRM and RRT, they&apos;re by no means optimal and can be quite jerky.</p>
<p align="center">
<img src="img/path-smoothing1.png" alt="drawing" width="600">
</p>
<p>Instead of using these paths directly, some post-processing can be applied to smooth out the paths and improve the results. One simple algorithm that can be used is often referred to as <strong>path shortcutter</strong>. It looks for ways to shorten the resulting path, by connecting two non-neighboring nodes together. If it&apos;s able to find a pair of nodes whose edge is collision-free, then the original path between the two nodes is replaced with the shortcut edge. If this process is successful, then a search algorithm like A* search can be applied to find the path from start to goal. The resultant path may not be optimal, but it proves that moving from start to goal is feasible.</p>
<p>See the video <a href="https://youtu.be/R20Mpz5y7-w">here</a>.</p>
<h3 class="mume-header" id="algorithm-2">Algorithm:</h3>

<p>The following algorithm provides a method for smoothing the path by shortcutting.</p>
<p>Keep in mind that the path&#x2019;s distance is not the only thing that can be optimized by the Path Shortcutter algorithm - it could optimize for path smoothness, expected energy use by the robot, safety, or any other measurable factor.</p>
<p>After the Path Shortcutting algorithm is applied, the result is a more optimized path. It may still not be the <em>optimal path</em>, but it should have at the very least moved towards a local minimum. There exist more complex, informed algorithms that can improve the performance of the Path Shortcutter. These are able to use information about the workspace to better guide the algorithm to a more optimal solution.</p>
<p>For large multi-dimensional problems, it is not uncommon for the time taken to optimize a path to exceed the time taken to search for a feasible solution in the first place.</p>
<p align="center">
<img src="img/path-smoothing-algo.png" alt="drawing" width="600">
</p>
<h1 class="mume-header" id="overall-concerns">Overall Concerns</h1>

<h3 class="mume-header" id="not-complete">Not Complete</h3>

<p>Sample-based planning is not complete, it is probabilistically complete. In applications where decisions need to be made quickly, PRM &amp; RRT may fail to find a path in difficult environments, such as the one shown below.</p>
<p align="center">
<img src="img/overall-concerns1.gif" alt="drawing" width="600">
</p>
<p>To path plan in an environment such as the one presented above, alternate means of sampling can be introduced (such as Gaussian or Bridge sampling). Alternate methods bias their placement of samples to obstacle edges or vertices of the open space.</p>
<h3 class="mume-header" id="not-optimal">Not Optimal</h3>

<p>Sample-based path planning isn&#x2019;t optimal either - while an algorithm such as A* will find the most optimal path within the graph, the graph is not a thorough representation of the space, and so the true optimal path is unlikely to be represented in the graph.</p>
<h3 class="mume-header" id="conclusion">Conclusion</h3>

<p>Overall, there is no silver bullet algorithm for sample-based path planning. The PRM &amp; RRT algorithms perform acceptably in most environments, while others require customized solutions. An algorithm that sees a performance improvement in one application, is not guaranteed to perform better in others.</p>
<p>Ultimately, sample-based path planning makes multi-dimensional path planning feasible!</p>
<h1 class="mume-header" id="sample-based-planning-wrap-up">Sample-Based Planning Wrap-Up</h1>

<p>Probabilistic roadmaps and rapidly exploring random trees are two alternate algorithms for path planning which are espacially applicable in large high-dimensional spaces. Although the algorithms are not complete, they are considered to be <strong>probabilistically complete</strong>, as their completeness grows exponentially with the number of samples collected. The algorithms that we learned here are incredibly applicable in real world robotics.</p>
<h3 class="mume-header" id="extended-reading">Extended Reading</h3>

<p>At this point, you have the knowledge to read through a paper on path planning. The following paper, <a href="https://www.cs.cmu.edu/~maxim/files/pathplanforMAV_icra13.pdf">Path Planning for Non-Circular Micro Aerial Vehicles in Constrained Environments</a>, addresses the problem of path planning for a quadrotor.</p>
<p>It is an enjoyable read that culminates the past two sections of path planning, as it references a number of planning methods that you have learned, and introduces a present-day application of path planning. Reading the paper will help you gain an appreciation of this branch of robotics, as well as help you gain confidence in the subject.</p>
<p>Some additional definitions that you may find helpful while reading the paper:</p>
<ul>
<li><strong>Anytime algorithm:</strong> an anytime algorithm is an algorithm that will return a solution even if it&apos;s computation is halted before it finishes searching the entire space. The longer the algorithm plans, the more optimal the solution will be.</li>
<li><em><em>RRT</em>:</em>* RRT* is a variant of RRT that tries to smooth the tree branches at every step. It does so by looking to see whether a child node can be swapped with it&apos;s parent (or it&apos;s parent&apos;s parent, etc) to produce a more direct path. The result is a less zig-zaggy and more optimal path.</li>
</ul>
<h1 class="mume-header" id="introduction-to-probabilistic-path-planning">Introduction to Probabilistic Path Planning</h1>

<p>Recall the exploratory rover introducted at the start of previous lesson.</p>
<p align="center">
<img src="img/path-planning-intro.png" alt="drawing" width="600">
</p>
<p>Its task was to find a path from its drop-off location to the goal location that would be safe for humans to follow. The terrain contains a lot of different hazards. The operator of the rover is willing to take whatever risk is necessary, but would naturally want to minimize it as much as possible. <strong>The algorithms that we&apos;ve learned thus far are unable to adequately model the risk</strong>. For instance, a combinatorial path planning algorithm would have no difficulty finding the path to the goal location. The path may be the best by all other means, but due to the uncertainty of the rover motion, there is a chance that the rover will meet its demise along this path.</p>
<p>It is possible to inflate the size of the rover to ensure that there is enough room to maneuver. But as we have seen, the algorithm will no longer be complete.</p>
<p>Another idea is to give negative rewards to dangerous areas of the map so that the search algorithm is more likely to select alternate paths.</p>
<p>Similar to what we do in reinforcement learning, this is a step in the right direction and would cause the rover to avoid dangerous areas. But, it does not actually consider the uncertainty of rover motion.</p>
<p>What we&apos;d really like is to model the uncertainty by considering a <em><strong>non-deterministic transition model</strong></em>. For instance, since the path execution is uncertain, an algorithm that takes the uncertainty into account explicitly is more likely to produce realistic paths. <strong>So, where does that leave us?</strong></p>
<p>We&apos;re back to <strong>Markov decision processes</strong>.</p>
<p>See the video <a href="https://youtu.be/CF6kz2H00ZU">here</a>.</p>
<h1 class="mume-header" id="markov-decision-process">Markov Decision Process</h1>

<h3 class="mume-header" id="recycling-robot-example">Recycling Robot Example</h3>

<p>Recall the recycling robot example that we covered in the Reinforcement Learning lesson. The robot&#x2019;s goal was to drive around its environment and pick up as many cans as possible. It had a set of <strong>states</strong> that it could be in, and a set of <strong>actions</strong> that it could take. The robot would receive a <strong>reward</strong> for picking up cans, however, it would also receive a negative reward (a penalty) if it were to run out of battery and get stranded.</p>
<p>The robot had a non-deterministic <strong>transition model</strong> (sometimes called the <em>one-step dynamics</em>). This means that an action cannot guarantee to lead a robot from one state to another state. Instead, there is a probability associated with resulting in each state.</p>
<p>Say at an arbitrary time step t, the state of the robot&apos;s battery is high (<code>S_t = high</code>). In response, the agent decides to search for cans (<code>A_t = search</code>). In such a case, there is a 70% chance of the robot&#x2019;s battery charge remaining high and a 30% chance that it will drop to low.</p>
<p>Let&#x2019;s revisit the definition of an MDP before moving forward.</p>
<h3 class="mume-header" id="mdp-definition">MDP Definition</h3>

<p>A Markov Decision Process is defined by:</p>
<ul>
<li>A set of states: <strong><code>S</code></strong>,</li>
<li>Initial state: <strong><code>s_0</code></strong>,</li>
<li>A set of actions: <strong><code>A</code></strong>,</li>
<li>The transition model: <strong><code>T(s,a,s&#x2032;)</code></strong>,</li>
<li>A set of rewards: <strong><code>R</code></strong>.</li>
</ul>
<p>The transition model is the probability of reaching a state <strong><code>s&apos;</code></strong> from a state <strong><code>s</code></strong> by executing action <strong><code>a</code></strong>.  It is often written as <strong><code>T(s,a,s&#x2032;)</code></strong>.</p>
<p>The Markov assumption states that the probability of transitioning from <strong><code>s</code></strong> to <strong><code>s&apos;</code></strong> is only dependent on the present state, <strong><code>s</code></strong>, and not on the path taken to get to <strong><code>s</code></strong>.</p>
<p>One notable difference between MDPs in probabilistic path planning and MDPs in reinforcement learning, is that in path planning the robot is fully aware of all of the items listed above (state, actions, transition model, rewards). Whereas in RL, the robot was aware of its state and what actions it had available, but it was not aware of the rewards or the transition model.</p>
<h3 class="mume-header" id="mobile-robot-example">Mobile Robot Example</h3>

<p>In our mobile robot example, movement actions are non-deterministic. Every action will have a probability less than 1 of being successfully executed. This can be due to a number of reasons such as wheel slip, internal errors, difficult terrain, etc. The image below showcases a possible transition model for our exploratory rover, for a scenario where it is trying to move forward one cell.</p>
<p align="center">
<img src="img/mdp1.png" alt="drawing" width="600">
</p>
<p>As you can see, the intended action of moving forward one cell is only executed with a probability of 0.8 (80%). With a probability of 0.1 (10%), the rover will move left, or right. Let&#x2019;s also say that bumping into a wall will cause the robot to remain in its present cell.</p>
<p>Let&#x2019;s provide the rover with a simple example of an environment for it to plan a path in. The environment shown below has the robot starting in the top left cell, and the robot&#x2019;s goal is in the bottom right cell. The mountains represent terrain that is more difficult to pass, while the pond is a hazard to the robot. Moving across the mountains will take the rover longer than moving on flat land, and moving into the pond may drown and short circuit the robot.</p>
<p align="center">
<img src="img/mdp2.gif" alt="drawing" width="600">
</p>
<h3 class="mume-header" id="combinatorial-path-planning-solution">Combinatorial Path Planning Solution</h3>

<p>If we were to apply A* search to this discretized 4-connected environment, the resultant path would have the robot move right 2 cells, then down 2 cells, and right once more to reach the goal (or R-R-D-R-D, which is an equally optimal path). This truly is the shortest path, however, it takes the robot right by a very dangerous area (the pond). There is a significant chance that the robot will end up in the pond, failing its mission.</p>
<p>If we are to path plan using MDPs, we might be able to get a better result!</p>
<h3 class="mume-header" id="probabilistic-path-planning-solution">Probabilistic Path Planning Solution</h3>

<p>In each state (cell), the robot will receive a certain reward, <strong><code>R(s)</code></strong>. This reward could be positive or negative, but it cannot be infinite. It is common to provide the following rewards,</p>
<ul>
<li>small negative rewards to states that are not the goal state(s) - to represent the cost of time passing (a slow moving robot would incur a greater penalty than a speedy robot),</li>
<li>large positive rewards for the goal state(s), and</li>
<li>large negative rewards for hazardous states - in hopes of convincing the robot to avoid them.</li>
</ul>
<p>These rewards will help guide the rover to a path that is efficient, but also safe - taking into account the uncertainty of the rover&#x2019;s motion.</p>
<p>The image below displays the environment with appropriate rewards assigned.</p>
<p align="center">
<img src="img/mdp3.png" alt="drawing" width="600">
</p>
<p>As you can see, entering a state that is not the goal state has a reward of -1 if it is a flat-land tile, and -3 if it is a mountainous tile. The hazardous pond has a reward of -50, and the goal has a reward of 100.</p>
<p>With the robot&#x2019;s transition model identified and appropriate rewards assigned to all areas of the environment, we can now construct a policy. Read on to see how that&#x2019;s done in probabilistic path planning!</p>
<h1 class="mume-header" id="policies">Policies</h1>

<p>Recall from the Reinforcement Learning lesson that a solution to a Markov Decision Process is called a policy, and is denoted with the letter <strong><code>&#x3C0;</code></strong>.</p>
<h3 class="mume-header" id="definition">Definition</h3>

<p>A <strong>policy</strong> is a mapping from states to actions. For every state, a policy will inform the robot of which action it should take. An <strong>optimal policy</strong>, denoted <strong><code>&#x3C0;&#x2217;</code></strong>, informs the robot of the <em>best</em> action to take from any state, to maximize the overall reward. We&#x2019;ll study optimal policies in more detail below.</p>
<p>If you aren&#x2019;t comfortable with policies, it is highly recommended that you return to the RL lesson and re-visit the sections that take you through the Gridworld Example, State-Value Functions, and Bellman Equations. These lessons demonstrate what a policy is, how state-value is calculated, and how the Bellman equations can be used to compute the optimal policy. These lessons also step you through a gridworld example that is <em>simpler</em> than the one you will be working with here, so it is wise to get acquainted with the RL example first.</p>
<h3 class="mume-header" id="developing-a-policy">Developing a Policy</h3>

<p>The image below displays the set of actions that the robot can take in its environment. Note that there are no arrows leading away from the pond, as the robot is considered DOA (dead on arrival) after entering the pond. As well, no arrows leave the goal as the path planning problem is complete once the robot reaches the goal - after all, this is an <em>episodic task</em>.</p>
<p align="center">
<img src="img/mdp4.png" alt="drawing" width="600">
</p>
<p>From this set of actions, a policy can be generated by selecting one action per state. Before we revisit the process of selecting the appropriate action for each policy, let&#x2019;s look at how some of the values above were calculated. After all, -5.9 seems like quite an odd number!</p>
<h3 class="mume-header" id="calculating-expected-rewards">Calculating Expected Rewards</h3>

<p>Recall that the reward for entering an empty cell is -1, a mountainous cell -3, the pond -50, and the goal +100. These are the rewards defined according to the environment. However, if our robot wanted to move from one cell to another, it it not guaranteed to succeed. Therefore, we must calculate the expected reward, which takes into account not just the rewards set by the environment, but the robot&apos;s transition model too.</p>
<p>Let&#x2019;s look at the bottom mountain cell first. From here, it is intuitively obvious that moving right is the best action to take, so let&#x2019;s calculate that one. If the robot&#x2019;s movements were deterministic, the cost of this movement would be trivial (moving to an open cell has a reward of -1). However, since our movements are non-deterministic, we need to evaluate the expected reward of this movement. The robot has a probability of 0.8 of successfully moving to the open cell, a probability of 0.1 of moving to the cell above, and a probability of 0.1 of bumping into the wall and remaining in its present cell.</p>
<p align="center">
<img src="img/mdp5.png" alt="drawing" width="500">
</p>
<p>All of the expected rewards are calculated in this way, taking into account the transition model for this particular robot.</p>
<p><em>You may have noticed that a few expected rewards are missing in the image above. Can you calculate their values?</em></p>
<p>Hopefully, after completing the quizzes, you are more comfortable with how the expected rewards are calculated. The image below has all of the expected rewards filled in.</p>
<p align="center">
<img src="img/mdp6.png" alt="drawing" width="500">
</p>
<h3 class="mume-header" id="selecting-a-policy">Selecting a Policy</h3>

<p>Now that we have an understanding of our expected rewards, we can select a policy and evaluate how efficient it is. Once again, a policy is just a mapping from states to actions. If we review the set of actions depicted in the image above, and select just one action for each state - i.e. exactly one arrow leaving each cell (with the exception of the hazard and goal states) - then we have ourselves a policy.</p>
<p>However, we&#x2019;re not looking for <em>any</em> policy, we&#x2019;d like to find the <em>optimal</em> policy. For this reason, we&#x2019;ll need to study the utility of each state to then determine the best action to take from each state. That&#x2019;s what the next concept is all about!</p>
<h1 class="mume-header" id="state-utility">State Utility</h1>

<h3 class="mume-header" id="definition-1">Definition</h3>

<p>The <strong>utility of a state</strong> (otherwise known as the <strong>state-value</strong>) represents how attractive the state is with respect to the goal. Recall that for each state, the state-value function yields the expected return, if the agent (robot) starts in that state and then follows the policy for all time steps. In mathematical notation, this can be represented as so:</p>
<p align="center">
<img src="img/mdp7.png" alt="drawing" width="300">
</p>
<p>The notation used in path planning differs slightly from what you saw in Reinforcement Learning. But the result is identical.</p>
<p>Here,</p>
<p align="left">
<img src="img/mdp8.png" alt="drawing" width="300">
</p>
<p>The utility of a state is the sum of the rewards that an agent would encounter if it started at that state and followed the policy to the goal.</p>
<h3 class="mume-header" id="calculation">Calculation</h3>

<p>We can break the equation down, to further understand it.</p>
<p align="center">
<img src="img/mdp9.png" alt="drawing" width="300">
</p>
<p>Let&#x2019;s start by breaking up the summation and explicitly adding all states.</p>
<p align="center">
<img src="img/mdp10.png" alt="drawing" width="400">
</p>
<p>Then, we can pull out the first term. The expected reward for the first state is independent of the policy. While the expected reward of all future states (those between the state and the goal) depend on the policy.</p>
<p align="center">
<img src="img/mdp11.png" alt="drawing" width="400">
</p>
<p>Re-arranging the equation results in the following. (Recall that the prime symbol, as on <strong><code>s&apos;</code></strong>, represents the next state - like <strong><code>s_2</code></strong> would be to <strong><code>s_1</code></strong>).</p>
<p align="center">
<img src="img/mdp12.png" alt="drawing" width="350">
</p>
<p>Ultimately, the result is the following,</p>
<p align="center">
<img src="img/mdp13.png" alt="drawing" width="220">
</p>
<p>As you see here, calculating the utility of a state is an iterative process. It involves all of the states that the agent would visit between the present state and the goal, as dictated by the policy.</p>
<p>As well, it should be clear that the utility of a state depends on the policy. If you change the policy, the utility of each state will change, since the sequence of states that would be visited prior to the goal may change.</p>
<h3 class="mume-header" id="determining-the-optimal-policy">Determining the Optimal Policy</h3>

<p>Recall that the <strong>optimal policy</strong>, denoted <strong><code>&#x3C0;&#x2217;</code></strong>, informs the robot of the best action to take from any state, to maximize the overall reward. That is,</p>
<p align="center">
<img src="img/mdp14.png" alt="drawing" width="220">
</p>
<p>In a state <strong><code>s</code></strong>, the optimal policy <strong><code>&#x3C0;&#x2217;</code></strong> will choose the action <strong><code>a</code></strong> that maximizes the utility of <strong><code>s</code></strong> (which, due to its iterative nature, maximizes the utilities of all future states too).</p>
<p>While the math may make it seem intimidating, it&#x2019;s as easy as looking at the set of actions and choosing the best action for every state. The image below displays the set of all actions once more.</p>
<p align="center">
<img src="img/mdp15.png" alt="drawing" width="600">
</p>
<p>It may not be clear from the get-go which action is optimal for every state, especially for states far away from the goal which have many paths available to them. It&#x2019;s often helpful to start at the goal and work your way backwards.</p>
<p>If you look at the two cells adjacent to the goal, their best action is trivial - go to the goal! Recall from your learning in RL that the goal state&#x2019;s utility is 0. This is because if the agent starts at the goal, the task is complete and no reward is received. Thus, the expected reward from either of the goal&#x2019;s adjacent cells is 79.8. Therefore, the state&#x2019;s utility is, 79.8 + 0 = 79.8, based on</p>
<p align="center">
<img src="img/mdp13.png" alt="drawing" width="200">
</p>
<p>If we look at the lower mountain cell, it is also easy to guess which action should be performed in this state. With an expected reward of -1.2, moving right is going to be much more rewarding than taking any indirect route (up or left). This state will have a utility of -1.2 + 79.8 = 78.6.</p>
<p><strong>Quiz: Can you calculate what would the utility of the state to the right of the center mountain be, if the most rewarding action is chosen?</strong></p>
<p align="center">
<img src="img/mdp16.png" alt="drawing" width="600">
</p>
<p>The process of selecting each state&#x2019;s most rewarding action continues, until every state is mapped to an action. These mappings are precisely what make up the policy.</p>
<p>It is highly suggested that you pause this lesson here, and work out the optimal policy on your own using the action set seen above. Working through the example yourself will give you a better understanding of the challenges that are faced in the process, and will help you remember this content more effectively. When you are done, you can compare your results with the images below.</p>
<h3 class="mume-header" id="applying-the-policy">Applying the Policy</h3>

<p>Once this process is complete, the agent (our robot) will be able to make the best path planning decision from every state, and successfully navigate the environment from any start position to the goal. The optimal policy for this environment and this robot is provided below.</p>
<p>The image below that shows the set of actions with just the optimal actions remaining. Note that from the top left cell, the agent could either go down or right, as both options have equal rewards.</p>
<p align="center">
<img src="img/mdp17.png" alt="drawing" width="600">
</p>
<p align="center">
<img src="img/mdp18.png" alt="drawing" width="600">
</p>
<p>Discounting<br>
One simplification that you may have noticed us make, is omit the discounting rate <strong><code>&#x3B3;</code></strong>. In the above example, <strong><code>&#x3B3;=1</code></strong> and all future actions were considered to be just as significant as the present action. This was done solely to simplify the example. After all, you have already been introduced to <strong><code>&#x3B3;</code></strong> through the lessons on Reinforcement Learning.</p>
<p>In reality, discounting is often applied in robotic path planning, since the future can be quite uncertain. The complete equation for the utility of a state is provided below:</p>
<p align="center">
<img src="img/mdp19.png" alt="drawing" width="300">
</p>
<h3 class="mume-header" id="value-iteration-algorithm">Value Iteration Algorithm</h3>

<p>The process that we went through to determine the optimal policy for the mountainous environment was fairly straightforward, but it did take some intuition to identify which action was optimal for every state. In larger more complex environments, intuition may not be sufficient. In such environments, an algorithm should be applied to handle all computations and find the optimal solution to an MDP. One such algorithm is called the Value Iteration algorithm. Iteration is a key word here, and you&#x2019;ll see just why!</p>
<p>The Value Iteration algorithm will initialize all state utilities to some arbitrary value - say, zero. Then, it will iteratively calculate a more accurate state utility for each state, using</p>
<p align="center">
<img src="img/mdp20.png" alt="drawing" width="300">
</p>
<h3 class="mume-header" id="algorithm-3">Algorithm</h3>

<p align="left">
<img src="img/mdp21.png" alt="drawing" width="400">
</p>
<p>With every iteration, the algorithm will have a more and more accurate estimate of each state&#x2019;s utility. The number of iterations of the algorithm is dictated by a function <em>close-enough</em> which detects convergence. One way to accomplish this is to evaluate the root mean square error,</p>
<p align="center">
<img src="img/mdp22.png" alt="drawing" width="300">
</p>
<p>Once this error is below a predetermined threshold, the result has converged sufficiently.</p>
<p align="center">
<img src="img/mdp23.png" alt="drawing" width="200">
</p>
<p>This algorithm finds the optimal policy to the MDP, regardless of what <strong><code>U&#x2032;</code></strong> is initialized to (although the efficiency of the algorithm will be affected by a poor <strong><code>U&#x2032;</code></strong>).</p>

      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>