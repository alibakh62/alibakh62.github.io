<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>gradient_boost.md</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__html"><h1 id="gradient-boost-for-regression">Gradient boost for regression</h1>
<p>Gradient Boost and AdaBoost are very similar. So, let’s first start by comparing the two algorithms.</p>
<h2 id="gradient-boost-vs.-adaboost">Gradient Boost vs. AdaBoost</h2>
<p>For a regression problem:</p>
<ul>
<li><strong>AdaBoost</strong> starts by:
<ul>
<li>Building a very short tree, called a <em><strong>stump</strong></em>, from the training data.</li>
<li>The amount of say the stump has on the final output is based on how well it compensated for those previous errors.</li>
<li>Then, AdaBoost builds the next stump based on errors that the previous stump made.</li>
<li>AdaBoost continues to make stumps in this fashion until it has made the number of stumps you asked for, or it has a perfect fit.</li>
</ul>
</li>
<li>In contrast, <strong>Gradient Boost (GB)</strong> start by:
<ul>
<li>Making a single leaf (instead of a tree or stump).</li>
<li>This leaf represents an initial guess for the <strong>target value</strong> for all samples.</li>
<li>When trying to predict a continuous value, <em><strong>the first guess is the average value</strong></em>.</li>
<li>Then, GB builds a tree. Like AdaBoost, this tree is based on the errors made by the previous tree.</li>
<li>Unlike AdaBoost, this tree is usually larger than a stump. However, GB still restricts the size of the tree (you specify the tree size by the <em><strong>number of leaves</strong></em>). In practice, the <strong>maximum number of leaves</strong> is set between <strong>8</strong> and <strong>32</strong>.<br>
Like AdaBoost, GB also scales the trees, but GB scales all the trees by the same amount (by <strong>learning rate</strong>), unlike AdaBoost.</li>
<li>Then, GB builds another tree based on the errors made by the previous tree and scales it.</li>
<li>GB continues to build trees in this fashion until it has made the number of trees you asked for, or additional trees fail to improve the fit.</li>
</ul>
</li>
</ul>
<h2 id="gradient-boost-algorithm">Gradient Boost algorithm</h2>
<ol>
<li>Calculate the average value of the target variable.</li>
<li>Next, we <strong>build a tree</strong> based on the error of the previous tree. The error is just the difference between the observed target and the predicted (average) target.
<ul>
<li><strong>Note:</strong> The difference is called <strong>pseudo residual</strong>. We build a tree to predict these residuals.</li>
<li><strong>Note:</strong> The trees can be different at each step.</li>
</ul>
</li>
<li>By restricting the number of leaves, we get a fewer number of leaves than the residuals. As a result, some residuals end up in the same leaf. <em><strong>We replace these residuals with their average value</strong></em>.</li>
<li>Now, we can combine the original leaf (average of target variable) with this tree to make a new prediction of the target variable. <em><strong>New prediction value = original prediction value + learning rate x prediction value from the tree</strong></em>.
<ul>
<li><strong>Learning rate:</strong> To control the contribution of trees and avoid overfitting the data, GB uses a learning rate. In other words, scaling the tree by the learning rate results in a small step in the right direction.</li>
<li>Main authors of GB suggested that empirical evidence shows that taking lots of small steps in the right direction results in better predictions with a testing dataset, i.e. lower variance.</li>
<li><strong>Note:</strong> At each step, to get the new prediction, we combine the results of <em><strong>ALL</strong></em> the predictions, i.e. <strong>new prediction = original prediction + (LR) x residuals tree1 + (LR) x residuals tree2 + (LR) x residuals tree3 + …</strong></li>
</ul>
</li>
<li>We repeat the steps above over and over again until we reach the maximum specified, or adding additional trees does not significantly reduce the size of residuals.</li>
<li></li>
</ol>
</div>
</body>

</html>
