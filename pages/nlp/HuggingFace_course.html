<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>HuggingFace_course.md</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
<style>
/* Style the button that is used to open and close the collapsible content */
.collapsible {
  background-color: #eee;
  color: #444;
  cursor: pointer;
  padding: 18px;
  width: 100%;
  border: none;
  text-align: left;
  outline: none;
  font-size: 15px;
}

/* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
.active, .collapsible:hover {
  background-color: #ccc;
}

/* Style the collapsible content. Note: hidden by default */
.content {
  padding: 0 18px;
  display: none;
  overflow: hidden;
  background-color: #f1f1f1;
}
</style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css">
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#what-is-nlp">What is NLP?</a></li>
<li><a href="#transformers-what-can-they-do">Transformers, what can they do?</a>
<ul>
<li><a href="#working-with-pipelines">Working with pipelines</a></li>
</ul>
</li>
<li><a href="#how-do-transformers-work">How do Transformers work?</a>
<ul>
<li><a href="#transformers-are-language-models">Transformers are language models</a></li>
<li><a href="#transformers-are-big-models">Transformers are big models</a></li>
<li><a href="#transfer-learning">Transfer Learning</a></li>
<li><a href="#transformers-general-architecture">Transformers: General Architecture</a></li>
<li><a href="#attention-layers">Attention Layers</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="what-is-nlp">What is NLP?</h1>
<p>The following is a list of common NLP tasks, with some examples of each:</p>
<ul>
<li><strong>Classifying whole sentences</strong>: Getting the sentiment of a review, detecting if an email is spam, determining if a sentence is grammatically correct or whether two sentences are logically related or not</li>
<li><strong>Classifying each word in a sentence</strong>: Identifying the grammatical components of a sentence (noun, verb, adjective), or the named entities (person, location, organization)</li>
<li><strong>Generating text content</strong>: Completing a prompt with auto-generated text, filling in the blanks in a text with masked words</li>
<li><strong>Extracting an answer from a text</strong>: Given a question and a context, extracting the answer to the question based on the information provided in the context</li>
<li><strong>Generating a new sentence from an input text</strong>: Translating a text into another language, summarizing a text</li>
</ul>
<h1 id="transformers-what-can-they-do">Transformers, what can they do?</h1>
<p>The <a href="https://github.com/huggingface/transformers">🤗 Transformers library</a> provides the functionality to create and use those shared models. The <a href="https://huggingface.co/models">Model Hub</a> contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub!</p>
<h2 id="working-with-pipelines">Working with pipelines</h2>
<p>The most basic object in the 🤗 Transformers library is the <code>pipeline()</code> function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:</p>
<pre class=" language-python"><code class="prism  language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

classifier <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"sentiment-analysis"</span><span class="token punctuation">)</span>
classifier<span class="token punctuation">(</span><span class="token string">"I've been waiting for a HuggingFace course my whole life."</span><span class="token punctuation">)</span>
</code></pre>
<p>By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the  <code>classifier</code>  object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.</p>
<p>There are three main steps involved when you pass some text to a pipeline:</p>
<ol>
<li>The text is preprocessed into a format the model can understand.</li>
<li>The preprocessed inputs are passed to the model.</li>
<li>The predictions of the model are post-processed, so you can make sense of them.</li>
</ol>
<p>Some of the currently  <a href="https://huggingface.co/transformers/main_classes/pipelines.html">available pipelines</a>  are:</p>
<ul>
<li><code>feature-extraction</code>  (get the vector representation of a text)</li>
<li><code>fill-mask</code></li>
<li><code>ner</code>  (named entity recognition)</li>
<li><code>question-answering</code></li>
<li><code>sentiment-analysis</code></li>
<li><code>summarization</code></li>
<li><code>text-generation</code></li>
<li><code>translation</code></li>
<li><code>zero-shot-classification</code></li>
</ul>
<p>You can specify options for <code>pipeline</code> and use models from the Model Hub:</p>
<pre class=" language-python"><code class="prism  language-python">generator <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span>
                      max_length<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">,</span>
                      num_return_sequences<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                      model<span class="token operator">=</span><span class="token string">'distilgpt2'</span><span class="token punctuation">)</span>
</code></pre>
<h1 id="how-do-transformers-work">How do Transformers work?</h1>
<p>The  <a href="https://arxiv.org/abs/1706.03762">Transformer architecture</a>  was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:</p>
<ul>
<li>
<p><strong>June 2018</strong>:  <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results</p>
</li>
<li>
<p><strong>October 2018</strong>:  <a href="https://arxiv.org/abs/1810.04805">BERT</a>, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)</p>
</li>
<li>
<p><strong>February 2019</strong>:  <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns</p>
</li>
<li>
<p><strong>October 2019</strong>:  <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a>, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT’s performance</p>
</li>
<li>
<p><strong>October 2019</strong>:  <a href="https://arxiv.org/abs/1910.13461">BART</a>  and  <a href="https://arxiv.org/abs/1910.10683">T5</a>, two large pretrained models using the same architecture as the original Transformer model (the first to do so)</p>
</li>
<li>
<p><strong>May 2020</strong>,  <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called  <em>zero-shot learning</em>)</p>
</li>
</ul>
<p><img src="https://huggingface.co/course/static/chapter1/transformers_chrono.png" alt="A brief chronology of Transformers models."></p>
<p>This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:</p>
<ul>
<li><mark>GPT-like (also called  <em>auto-regressive</em>  Transformer models)</mark></li>
<li><mark>BERT-like (also called  <em>auto-encoding</em>  Transformer models)</mark></li>
<li><mark>BART/T5-like (also called  <em>sequence-to-sequence</em>  Transformer models)</mark></li>
</ul>
<h2 id="transformers-are-language-models">Transformers are language models</h2>
<p>All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as  <mark><em>language models</em></mark>. This means they have been trained on large amounts of raw text in a <mark>self-supervised</mark> fashion. Self-supervised learning is a type of training in which the objective is automatically computed from the inputs of the model. That means that humans are not needed to label the data!</p>
<p><mark>This type of model develops a statistical understanding of the language it has been trained on, but it’s not very useful for specific practical tasks.</mark> Because of this, the general <mark>pretrained</mark> model then goes through a process called  <mark><em>transfer learning</em></mark>. During this process, the <mark>model is fine-tuned in a supervised way</mark> — that is, using human-annotated labels — on a given task.</p>
<p>An example of a task is predicting the next word in a sentence having read the  <em>n</em>  previous words. This is called  <mark><em>causal language modeling</em></mark>  because the output depends on the past and present inputs, but not the future ones.</p>
<center><img src="https://huggingface.co/course/static/chapter1/causal_modeling.png" width="80%" height="auto">
</center><p>Another example is  <mark><em>masked language modeling</em></mark>, in which the model predicts a masked word in the sentence.</p>
<center><img src="https://huggingface.co/course/static/chapter1/masked_modeling.png" width="80%" height="auto">
</center><h2 id="transformers-are-big-models">Transformers are big models</h2>
<p>Apart from a few outliers (like DistilBERT), the general strategy to achieve better performance is by increasing the models’ sizes as well as the amount of data they are pretrained on.</p>
<center><img src="https://huggingface.co/course/static/chapter1/model_parameters.png" width="80%" height="auto">
</center><h2 id="transfer-learning">Transfer Learning</h2>
<p><mark><em>Pretraining</em></mark>  is the act of training a model from scratch: the weights are randomly initialized, and the training starts without any prior knowledge.</p>
<center><img src="https://huggingface.co/course/static/chapter1/pretraining.png" width="80%" height="auto">
</center><p>This pretraining is usually done on very <strong>large amounts of data</strong>. Therefore, it requires a very large corpus of data, and training can take up to several weeks.</p>
<p><mark><em>Fine-tuning</em></mark>, on the other hand, is the training done  <strong>after</strong>  a model has been pretrained. To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task. Wait — <mark>why not simply train directly for the final task?</mark> There are a couple of reasons:</p>
<ul>
<li>The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of knowledge acquired by the initial model during pretraining (for instance, with NLP problems, <mark>the pretrained model will have some kind of statistical understanding of the language you are using for your task</mark>).</li>
<li><mark>Since the pretrained model was already trained on lots of data, the fine-tuning requires way less data to get decent results.</mark></li>
<li>For the same reason, the amount of time and resources needed to get good results are much lower.</li>
</ul>
<p>For example, one could leverage a pretrained model trained on the English language and then fine-tune it on an arXiv corpus, resulting in a science/research-based model. The fine-tuning will only require a limited amount of data: the knowledge the pretrained model has acquired is “transferred,” hence the term  <em>transfer learning</em>.</p>
<center><img src="https://huggingface.co/course/static/chapter1/finetuning.png" width="80%" height="auto">
</center><p>Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining.</p>
<p><mark><strong>This process will also achieve better results than training from scratch</strong></mark> (unless you have lots of data), which is why <mark>you should always try to leverage a pretrained model</mark> — one as close as possible to the task you have at hand — and fine-tune it.</p>
<h2 id="transformers-general-architecture">Transformers: General Architecture</h2>
<p>The model is primarily composed of two blocks:</p>
<ul>
<li><strong>Encoder (left)</strong>: The encoder receives an input and builds a representation of it (its features). This means that the model is <mark>optimized to acquire understanding from the input</mark>.</li>
<li><strong>Decoder (right)</strong>: The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is <mark>optimized for generating outputs</mark>.</li>
</ul>
<center><img src="https://huggingface.co/course/static/chapter1/transformers_blocks.png" width="40%" height="auto">
</center><p>Each of these parts can be used independently, depending on the task:</p>
<ul>
<li><strong>Encoder-only models</strong>: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.</li>
<li><strong>Decoder-only models</strong>: Good for generative tasks such as text generation.</li>
<li><strong>Encoder-decoder models</strong>  or  <strong>sequence-to-sequence models</strong>: Good for generative tasks that require an input, such as translation or summarization.</li>
</ul>
<h2 id="attention-layers">Attention Layers</h2>
<p>This layer will tell the model to pay specific attention to certain words in the sentence you passed it (and more or less ignore the others) when dealing with the representation of each word.</p>

    </div>
  </div>
  <script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.display === "block") {
      content.style.display = "none";
    } else {
      content.style.display = "block";
    }
  });
}
</script>
<script src="https://gist.github.com/username/a39a422ebdff6e732753b90573100b16.js"></script>
</body>

</html>
