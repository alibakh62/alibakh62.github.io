<!DOCTYPE html><html><head>
      <title>project3_continuous_control</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:////Users/abakh005/.vscode/extensions/shd101wyy.markdown-preview-enhanced-0.6.7/node_modules/@shd101wyy/mume/dependencies/katex/katex.min.css">
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p,html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div{display:inline}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  300px/2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <h1 class="mume-header" id="-unity-ml-agents">Unity ML-Agents</h1>

<p><strong>Unity Machine Learning Agents (ML-Agents)</strong>  is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents.</p>
<p>For game developers, these trained agents can be used for multiple purposes, including controlling  <a href="https://en.wikipedia.org/wiki/Non-player_character">NPC</a>  behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release.</p>
<p>In this course, you will use Unity&apos;s rich environments to design, train, and evaluate your own deep reinforcement learning algorithms. You can read more about ML-Agents by perusing the  <a href="https://github.com/Unity-Technologies/ml-agents">GitHub repository</a>.</p>
<blockquote>
<p><strong>Note: The Unity ML-Agent team frequently releases updated versions of their environment. We are using the v0.4 interface. To avoid any confusion, please use the workspace we provide here or work with v0.4 locally.</strong></p>
</blockquote>
<p><img src="https://video.udacity-data.com/topher/2018/April/5ad8b114_2018-02-27-16-05-37/2018-02-27-16-05-37.gif" alt="Winner of the Unity ML-Agents Challenge: A robotic arm that can make pancakes!"></p>
<p>Winner of the Unity ML-Agents Challenge: A robotic arm that can make pancakes!</p>
<h1 class="mume-header" id="-the-environment">The Environment</h1>

<p>For this project, you will work with the  <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher">Reacher</a>  environment.</p>
<p><img src="https://video.udacity-data.com/topher/2018/June/5b1ea778_reacher/reacher.gif" alt="Unity ML-Agents Reacher Environment"></p>
<p>Unity ML-Agents Reacher Environment</p>
<p>In this environment, a double-jointed arm can move to target locations. A reward of +0.1 is provided for each step that the agent&apos;s hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.</p>
<p>The observation space consists of 33 variables corresponding to position, rotation, velocity, and angular velocities of the arm. Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.</p>
<h2 class="mume-header" id="-distributed-training">Distributed Training</h2>

<hr>
<p>For this project, we will provide you with two separate versions of the Unity environment:</p>
<ul>
<li>The first version contains a single agent.</li>
<li>The second version contains 20 identical agents, each with its own copy of the environment.</li>
</ul>
<p>The second version is useful for algorithms like  <a href="https://arxiv.org/pdf/1707.06347.pdf">PPO</a>,  <a href="https://arxiv.org/pdf/1602.01783.pdf">A3C</a>, and  <a href="https://openreview.net/pdf?id=SyZipzbCb">D4PG</a>  that use multiple (non-interacting, parallel) copies of the same agent to distribute the task of gathering experience.</p>
<h2 class="mume-header" id="-solving-the-environment">Solving the Environment</h2>

<hr>
<p>Note that your project submission need only solve one of the two versions of the environment.</p>
<h3 class="mume-header" id="-option-1-solve-the-first-version">Option 1: Solve the First Version</h3>

<p>The task is episodic, and in order to solve the environment, your agent must get an average score of +30 over 100 consecutive episodes.</p>
<h3 class="mume-header" id="-option-2-solve-the-second-version">Option 2: Solve the Second Version</h3>

<p>The barrier for solving the second version of the environment is slightly different, to take into account the presence of many agents. In particular, your agents must get an average score of +30 (over 100 consecutive episodes, and over all agents). Specifically,</p>
<ul>
<li>After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 20 (potentially different) scores. We then take the average of these 20 scores.</li>
<li>This yields an  <strong>average score</strong>  for each episode (where the average is over all 20 agents).</li>
</ul>
<p>As an example, consider the plot below, where we have plotted the  <strong>average score</strong>  (over all 20 agents) obtained with each episode.</p>
<p><img src="https://video.udacity-data.com/topher/2018/July/5b48f845_unknown/unknown.png" alt="Plot of average scores (over all agents) with each episode."></p>
<p>Plot of average scores (over all agents) with each episode.</p>
<p>The environment is considered solved, when the average (over 100 episodes) of those  <strong>average scores</strong>  is at least +30. In the case of the plot above, the environment was solved at episode 63, since the average of the  <strong>average scores</strong>  from episodes 64 to 163 (inclusive) was greater than +30.</p>
<h1 class="mume-header" id="-the-environment-1">The Environment</h1>

<p>As you&apos;ve seen, the environment for this project involves controlling a double-jointed arm, to reach target locations.</p>
<h2 class="mume-header" id="-real-world-robotics">Real-World Robotics</h2>

<hr>
<p>Watch this  <a href="https://www.youtube.com/watch?v=ZVIxt2rt1_4">YouTube video</a>  to see how some researchers were able to train a similar task on a real robot! The accompanying research paper can be found  <a href="https://arxiv.org/pdf/1803.07067.pdf">here</a>.</p>
<p><img src="https://video.udacity-data.com/topher/2018/May/5b0c72f3_output/output.gif" alt="Training robotic arm to reach target locations in the real world."></p>
<p>Training robotic arm to reach target locations in the real world. (<a href="https://www.youtube.com/watch?v=ZVIxt2rt1_4">Source</a>)</p>
<h2 class="mume-header" id="-sharing-experience">Sharing Experience</h2>

<hr>
<p>In the second version of the project environment, there are 20 identical copies of the agent. It has been shown that having multiple copies of the same agent  <a href="https://ai.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html">sharing experience can accelerate learning</a>, and you&apos;ll discover this for yourself when solving the project!</p>
<p><img src="https://video.udacity-data.com/topher/2018/June/5b35985b_image8/image8.gif" alt="Sharing experience can accelerate learning."></p>
<p>Sharing experience can accelerate learning. (<a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html">Source</a>)</p>
<h1 class="mume-header" id="-the-environment-2">The Environment</h1>

<p>Follow the instructions below to explore the environment on your own machine! You will also learn how to use the Python API to control your agent.</p>
<h2 class="mume-header" id="-step-1-activate-the-environment">Step 1: Activate the Environment</h2>

<hr>
<p>If you haven&apos;t already, please follow the  <a href="https://github.com/udacity/deep-reinforcement-learning#dependencies">instructions in the DRLND GitHub repository</a>  to set up your Python environment. These instructions can be found in  <code>README.md</code>  at the root of the repository. By following these instructions, you will install PyTorch, the ML-Agents toolkit, and a few more Python packages required to complete the project.</p>
<p>(<em>For Windows users</em>) The ML-Agents toolkit supports Windows 10. While it might be possible to run the ML-Agents toolkit using other versions of Windows, it has not been tested on other versions. Furthermore, the ML-Agents toolkit has not been tested on a Windows VM such as Bootcamp or Parallels.</p>
<h2 class="mume-header" id="-step-2-download-the-unity-environment">Step 2: Download the Unity Environment</h2>

<hr>
<p>For this project, you will  <strong>not</strong>  need to install Unity - this is because we have already built the environment for you, and you can download it from one of the links below. You need only select the environment that matches your operating system:</p>
<h3 class="mume-header" id="-version-1-one-1-agent">Version 1: One (1) Agent</h3>

<ul>
<li>Linux:  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux.zip">click here</a></li>
<li>Mac OSX:  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher.app.zip">click here</a></li>
<li>Windows (32-bit):  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86.zip">click here</a></li>
<li>Windows (64-bit):  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Windows_x86_64.zip">click here</a></li>
</ul>
<h3 class="mume-header" id="-version-2-twenty-20-agents">Version 2: Twenty (20) Agents</h3>

<ul>
<li>Linux:  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip">click here</a></li>
<li>Mac OSX:  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher.app.zip">click here</a></li>
<li>Windows (32-bit):  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86.zip">click here</a></li>
<li>Windows (64-bit):  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Windows_x86_64.zip">click here</a></li>
</ul>
<p>Then, place the file in the  <code>p2_continuous-control/</code>  folder in the DRLND GitHub repository, and unzip (or decompress) the file.</p>
<p>(<em>For Windows users</em>) Check out  <a href="https://support.microsoft.com/en-us/help/827218/how-to-determine-whether-a-computer-is-running-a-32-bit-version-or-64">this link</a>  if you need help with determining if your computer is running a 32-bit version or 64-bit version of the Windows operating system.</p>
<p>(<em>For AWS</em>) If you&apos;d like to train the agent on AWS (and have not  <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md">enabled a virtual screen</a>), then please use  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/one_agent/Reacher_Linux_NoVis.zip">this link</a>  (version 1) or  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux_NoVis.zip">this link</a>  (version 2) to obtain the &quot;headless&quot; version of the environment. You will  <strong>not</strong>  be able to watch the agent without enabling a virtual screen, but you will be able to train the agent. (<em>To watch the agent, you should follow the instructions to  <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md">enable a virtual screen</a>, and then download the environment for the  <strong>Linux</strong>  operating system above.</em>)</p>
<h2 class="mume-header" id="-step-3-explore-the-environment">Step 3: Explore the Environment</h2>

<hr>
<p>After you have followed the instructions above, open  <code>Continuous_Control.ipynb</code>  (located in the  <code>p2_continuous-control/</code>  folder in the DRLND GitHub repository) and follow the instructions to learn how to use the Python API to control the agent.</p>
<p>Watch the (<em>silent</em>) video below to see what kind of output to expect from the notebook (for version 2 of the environment), if everything is working properly! Version 1 will look very similar (where you&apos;ll see a single agent, instead of 20!).</p>
<p>Watch <a href="https://youtu.be/i2gVvXgOMnc">this video</a>.</p>
<p>In the last code cell of the notebook, you&apos;ll learn how to design and observe an agent that always selects random actions at each timestep. Your goal in this project is to create an agent that performs much better!</p>
<h2 class="mume-header" id="-optional-build-your-own-environment">(Optional) Build your Own Environment</h2>

<hr>
<p>For this project, we have built the Unity environment for you, and you must use the environment files that we have provided.</p>
<p>If you are interested in learning to build your own Unity environments  <strong>after completing the project</strong>, you are encouraged to follow the instructions  <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md">here</a>, which walk you through all of the details of building an environment from a Unity scene.</p>
<h1 class="mume-header" id="-project-instructions">Project Instructions</h1>

<p>For this project, you will train an agent to solve the provided environment.</p>
<p>To submit the project, you will provide a link to a GitHub repository with your implementation. If you would like a refresher on GitHub, please check out the lessons on GitHub in the extracurricular content.</p>
<p>To review the detailed project requirements, please read the  <a href="https://review.udacity.com/#!/rubrics/1890/view">project rubric</a>.</p>
<p>The format of this project is largely open-ended; you need only satisfy the points in the rubric. For instance, while we suspect that the majority of students will train the agent in a Jupyter notebook, you are welcome to instead structure your repository so that your Python code is run from the command line instead.</p>
<h2 class="mume-header" id="-your-github-submission">Your GitHub Submission</h2>

<hr>
<p>As described in the rubric, your GitHub submission should contain:</p>
<ul>
<li>a  <strong>README</strong>  that describes how someone not familiar with this project should use your repository. The README should be designed for a general audience that may not be familiar with the Nanodegree program; you should describe the environment that you solved, along with how to install the requirements before running the code in your repository.</li>
<li>the  <strong>code</strong>  that you use for training the agent, along with the trained model weights.</li>
<li>a  <strong>report</strong>  describing your learning algorithm. This is where you will describe the details of your implementation, along with ideas for future work.</li>
</ul>
<p>This GitHub repository will serve as a portfolio piece to share your new skills with the global community of reinforcement learning students and practitioners, along with potential employers!</p>
<h2 class="mume-header" id="-project-workspace">Project Workspace</h2>

<hr>
<p>While you are welcome to train the agent locally on your own machine, you can also complete the project in the  <strong>Workspace</strong>  that appears towards the end of this lesson. Note that the Workspace does not allow you to see the simulator of the environment; so, if you want to watch the agent while it is training, you should train locally.</p>
<p>The Workspace provides a Jupyter server directly in your browser and has GPU support. You can learn more about the Workspace by perusing the  <strong>Udacity Workspaces</strong>  lesson in the extracurricular content.</p>
<h1 class="mume-header" id="-benchmark-implementation">Benchmark Implementation</h1>

<p>For this project, you can use any algorithm of your choosing to solve the task. You are strongly encouraged to do your own research, to devise your own approach towards solving this problem.</p>
<p>In case you get stuck, here are the details of one approach that worked well for us.</p>
<h2 class="mume-header" id="-an-amended-ddpg-agent">An Amended DDPG Agent</h2>

<hr>
<p>In this part of the Nanodegree program, you learned about a lot of potential ways to solve this project. We instead decided to solve the project by making some amendments to the Deep Deterministic Policy Gradients (DDPG) algorithm.</p>
<h3 class="mume-header" id="-attempt-1">Attempt 1</h3>

<p>The first thing that we did was amend the DDPG code to work for multiple agents, to solve version 2 of the environment. The DDPG code in the DRLND GitHub repository utilizes only a single agent, and with each step:</p>
<ul>
<li>the agent adds its experience to the replay buffer, and</li>
<li>the (local) actor and critic networks are updated, using a sample from the replay buffer.</li>
</ul>
<p>So, in order to make the code work with 20 agents, we modified the code so that after each step:</p>
<ul>
<li>each agent adds its experience to a replay buffer that is shared by all agents, and</li>
<li>the (local) actor and critic networks are updated 20 times in a row (one for each agent), using 20 different samples from the replay buffer.</li>
</ul>
<p>In hindsight, this wasn&apos;t a great plan, but it was a start! That said, the scores are shown below.</p>
<p><img src="https://video.udacity-data.com/topher/2018/July/5b4cbfa1_screen-shot-2018-05-02-at-4.56.45-pm/screen-shot-2018-05-02-at-4.56.45-pm.png" alt></p>
<p>You&apos;ll notice that we made some rapid improvement pretty early in training, because of the extremely large number of updates. Unfortunately, also due to the large number of updates, the agent is incredibly unstable. Around episode 100, performance crashed and did not recover.</p>
<p>So, we focused on determining ways to stabilize this first attempt.</p>
<h3 class="mume-header" id="-attempt-2">Attempt 2</h3>

<p>For this second attempt, we reduced the number of agents from 20 to 1 (by switching to version 1 of the environment). We wanted to know how much stability we could expect from a single agent. The idea was that the code would likely train more reliably, if we didn&apos;t make so many updates. And it did train much better.</p>
<p><img src="https://video.udacity-data.com/topher/2018/July/5b4cc314_screen-shot-2018-05-03-at-9.10.50-am/screen-shot-2018-05-03-at-9.10.50-am.png" alt></p>
<p>At one point, we even hit the target score of 30. However, this score wasn&apos;t maintained for very long, and we saw strong indications that the algorithm was going to crash again. This showed us that we needed to spend more time with figuring out how to stabilize the algorithm, if we wanted to have a chance of training all 20 agents simultaneously.</p>
<h3 class="mume-header" id="-attempt-3">Attempt 3</h3>

<p>This time, we switched back to version 2 of the environment, and began with the code from  <strong>Attempt 1</strong>  as a starting point. Then, the only change we made was to use gradient clipping when training the critic network. The corresponding snippet of code was as follows:</p>
<pre data-role="codeBlock" data-info class="language-"><code>self.critic_optimizer.zero_grad()
critic_loss.backward()
torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)
self.critic_optimizer.step()

</code></pre><p>The corresponding scores are plotted below.</p>
<p><img src="https://video.udacity-data.com/topher/2018/July/5b4cc98c_pic3/pic3.png" alt></p>
<p>This is when we really started to feel hopeful. We still didn&apos;t maintain an average score of 30 over 100 episodes, but we maintained the score for longer than before. And the agent didn&apos;t crash as suddenly as in the previous attempts!</p>
<h3 class="mume-header" id="-attempt-4">Attempt 4</h3>

<p>At this point, we decided to get less aggressive with the number of updates per time step. In particular, instead of updating the actor and critic networks  <strong>20 times</strong>  at  <strong>every timestep</strong>, we amended the code to update the networks  <strong>10 times</strong>  after every  <strong>20 timesteps</strong>. The corresponding scores are plotted below.</p>
<p><img src="https://video.udacity-data.com/topher/2018/July/5b48f845_unknown/unknown.png" alt></p>
<p>And, this was enough to solve the environment! In hindsight, we probably should have realized this fix much earlier, but this long path to the solution was definitely a nice way to help with building intuition! &#x1F603;</p>
<h2 class="mume-header" id="-note">Note</h2>

<hr>
<p>If you are interested in implementing a method that will be more stable with the project, please explore  <a href="https://arxiv.org/abs/1604.06778">this paper</a>. As discussed in the paper, Trust Region Policy Optimization (TRPO) and Truncated Natural Policy Gradient (TNPG) should achieve better performance. You may also like to write your own implementation of Proximal Policy Optimization (PPO), which has also  <a href="https://blog.openai.com/openai-baselines-ppo/">demonstrated good performance</a>  with continuous control tasks.</p>
<p>You may also like to explore the (very!) recent  <a href="https://openreview.net/forum?id=SyZipzbCb">Distributed Distributional Deterministic Policy Gradients (D4PG)</a>  algorithm as another method for adapting DDPG for continuous control.</p>
<h1 class="mume-header" id="-not-sure-where-to-start">Not sure where to start?</h1>

<p>If you&apos;re not sure where to start, here are some suggestions for how to make some progress with the project. You need not follow this advice; these are only suggestions, and you should follow whatever path works best for you!</p>
<h2 class="mume-header" id="-step-1-master-the-details-of-the-deep-deterministic-policy-gradients-ddpg-algorithm">Step 1: Master the details of the Deep Deterministic Policy Gradients (DDPG) algorithm.</h2>

<hr>
<p>Read the  <a href="https://arxiv.org/abs/1509.02971">DDPG paper</a>  to master all of the details. Focus on the information in  <strong>3. Algorithm</strong>  and  <strong>7. Experiment Details</strong>  to learn how to adapt the implementation for your task. Refer to the lesson on  <strong>Actor-Critic Methods</strong>  to cement your understanding. If you have any questions, post them in Slack!</p>
<h2 class="mume-header" id="-step-2-study-the-coding-exercise-from-the-lesson">Step 2: Study the coding exercise from the lesson.</h2>

<hr>
<p>In the  <strong>Actor-Critic Methods</strong>  lesson, you applied a DDPG implementation to an OpenAI Gym task. Take the time to understand this code in great detail. Tweak the various hyperparameters and settings to build your intuition for what should work well (<em>and what doesn&apos;t!</em>).</p>
<h2 class="mume-header" id="-step-3-adapt-the-code-from-the-lesson-to-the-project">Step 3: Adapt the code from the lesson to the project.</h2>

<hr>
<p>Adapt the code from the exercise to the project, while making as few modifications as possible. Don&apos;t worry about efficiency, and just make sure the code runs. Don&apos;t worry about modifying hyperparameters, optimizers, or anything else of that nature just yet.</p>
<p>For this step, you do not need to run your code on a GPU.  <strong>In particular, if working in the Udacity-provided Workspace, GPU should not be enabled.</strong>  Save your GPU hours for the next step!</p>
<h2 class="mume-header" id="-step-4-optimize-the-hyperparameters">Step 4: Optimize the hyperparameters.</h2>

<hr>
<p>After you have verified that your DDPG code runs, try a few long training sessions while running your code on CPU. If your agent fails to learn, try out a few potential solutions by modifying your code. Once you&apos;re feeling confident (<em>or impatient &#x1F603;</em>) try out your implementation with GPU support!</p>
<h2 class="mume-header" id="-step-5-continue-to-explore">Step 5: Continue to explore!</h2>

<hr>
<p>Read  <a href="https://arxiv.org/abs/1604.06778">this paper</a>, which evaluates the performance of various deep RL algorithms on continuous control tasks. The paper introduces REINFORCE, TNPG, RWR, REPS, TRPO, CEM, CMA-ES and DDPG, and provides some useful suggestions that will help you to figure out which are best suited for the project.</p>
<h1 class="mume-header" id="-general-advice">General Advice</h1>

<p>In deep RL, failure comes in many flavors. Sometimes your agent doesn&apos;t train  <strong><em>at all</em></strong>. Sometimes it  <strong><em>appears</em></strong>  to be learning, but then  <em>something weird happens</em>  late in training that completely derails its performance.</p>
<p>When this happens, remember that  <strong>you are part of a community</strong>. Please reach out to your fellow students and mentors in Student Hub, ask for advice in Knowledge, submit your project to get feedback from a reviewer. We will figure this out together! Deep RL isn&apos;t easy and arguably  <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">&quot;doesn&apos;t work yet&quot;</a>. A large part of mastering cutting-edge algorithms is banging your head against the table - and we&apos;ll be right there with you. We promise that it will be so incredibly rewarding when you ultimately get it working!</p>
<h2 class="mume-header" id="-failure-is-inevitable-and-only-temporary">Failure is Inevitable and Only Temporary</h2>

<hr>
<p>When we wrote a solution to this project, we didn&apos;t get it right the first time. Or the second. Or the third. &#x1F603;</p>
<p><img src="https://video.udacity-data.com/topher/2018/July/5b462e53_screen-shot-2018-07-11-at-11.19.56-am/screen-shot-2018-07-11-at-11.19.56-am.png" alt="Failed attempts at training the project. (Remember solving is 30!)"></p>
<p>Failed attempts at training the project. (Remember solving is 30!)</p>
<h1 class="mume-header" id="-optional-challenge-crawl">(Optional) Challenge: Crawl</h1>

<p>After you have successfully completed the project, you might like to solve a more difficult continuous control environment, where the goal is to teach a creature with four legs to walk forward without falling.</p>
<p>You can read more about this environment in the ML-Agents GitHub  <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#crawler">here</a>.</p>
<p><img src="https://video.udacity-data.com/topher/2018/August/5b633811_crawler/crawler.png" alt="ML-Agents Crawler Environment"></p>
<p>ML-Agents Crawler Environment</p>
<h2 class="mume-header" id="-download-the-unity-environment">Download the Unity Environment</h2>

<hr>
<p>To solve this harder task, you&apos;ll need to download a new Unity environment. You need only select the environment that matches your operating system:</p>
<ul>
<li>Linux:  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Crawler/Crawler_Linux.zip">click here</a></li>
<li>Mac OSX:  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Crawler/Crawler.app.zip">click here</a></li>
<li>Windows (32-bit):  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Crawler/Crawler_Windows_x86.zip">click here</a></li>
<li>Windows (64-bit):  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Crawler/Crawler_Windows_x86_64.zip">click here</a></li>
</ul>
<p>Then, place the file in the  <code>p2_continuous-control/</code>  folder in the DRLND GitHub repository, and unzip (or decompress) the file.</p>
<blockquote>
<p>Please do not submit a project with this new environment. You are  <strong>required</strong>  to complete the project with the Reacher environment that was provided earlier in this lesson, in  <strong>The Environment - Explore</strong>.</p>
</blockquote>
<p>(<em>For AWS</em>) If you&apos;d like to train the agent on AWS (and have not  <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md">enabled a virtual screen</a>), then please use  <a href="https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Crawler/Crawler_Linux_NoVis.zip">this link</a>  to obtain the &quot;headless&quot; version of the environment. You will  <strong>not</strong>  be able to watch the agent without enabling a virtual screen, but you will be able to train the agent. (<em>To watch the agent, you should follow the instructions to  <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-on-Amazon-Web-Service.md">enable a virtual screen</a>, and then download the environment for the  <strong>Linux</strong>  operating system above.</em>)</p>
<h2 class="mume-header" id="-explore-the-environment">Explore the Environment</h2>

<hr>
<p>After you have followed the instructions above, open  <code>Crawler.ipynb</code>  (located in the  <code>p2_continuous-control/</code>  folder in the DRLND GitHub repository) and follow the instructions to learn how to use the Python API to control the agent.</p>

      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>