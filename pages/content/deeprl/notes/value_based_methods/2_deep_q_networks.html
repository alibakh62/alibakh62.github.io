<!DOCTYPE html><html><head>
      <title>2_deep_q_networks</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:////Users/abakh005/.vscode/extensions/shd101wyy.markdown-preview-enhanced-0.6.7/node_modules/@shd101wyy/mume/dependencies/katex/katex.min.css">
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p,html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div{display:inline}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  300px/2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <h1 class="mume-header" id="-from-rl-to-deep-rl">From RL to Deep RL</h1>

<p>So far, you&apos;ve solved many of your own reinforcement learning problems, using solution methods that represent the action values in a small table. Earlier in the nanodegree, we referred to this table as a  <strong>Q-table</strong>.</p>
<p>In the video below,  <strong>Kelvin Lwin</strong>  will introduce you to the idea of using neural networks to expand the size of the problems that we can solve with reinforcement learning. This context is useful preparation for exploring the details behind the Deep Q-Learning algorithm later in this lesson!</p>
<p><em>Kelvin is a Senior Deep Learning Instructor at the  <a href="https://www.nvidia.com/en-us/deep-learning-ai/education">NVIDIA Deep Learning Institute</a>.</em></p>
<p>RL is a branch of machine learning where an agent outputs an action and the environment returns an observation or the state of the system and a reward.</p>
<p>The <strong>goal of an agent</strong> is to best determine the best action to take. Usually, RL is described in terms of this agent interacting with the previously unknown environment, trying to maximize the overall or total reward. Now then, <strong>what is Deep RL?</strong></p>
<p>In some sense, it is using nonlinear function approximators to calculate the value actions based directly on observations from the environment. We represented it as a <strong>Deep Neural Network</strong>. We then deep learning to find the optimal parameters for these function approximators.</p>
<p>You have already worked with some deep learning neural networks for classification, detection, and semantic segmentation. However, these deep learning applications use labeled training data for supervised learning. The inference engine then produces the best guess label, not an action, as the output.</p>
<p>When an RL agent handles the entire end-to-end pipeline, it&apos;s called <strong>pixels-to-action</strong>, referring to the network&apos;s ability to take raw sensor data and choose the action it thinks will best maximize its reward.</p>
<p>Overtime, RL agents have a uncanny knack for developing intuitive human-like behaviors like learning to walk or peeking behind corners when they&apos;re unsure. They naturally incorporate elements of exploration and knowledge gathering, which makes them good for imitating behaviors and performing path planning.</p>
<p>Robots operating in unstructured environments tend to greatly benefit from RL agents, which gives them a way to make sense of the environment, which can be hard to model in advance.</p>
<p>See the video <a href="https://youtu.be/7HLJ0uaR1F0">here</a>.</p>
<p>As you&apos;ll learn in this lesson, the Deep Q-Learning algorithm represents the optimal action-value function <img src="https://latex.codecogs.com/gif.latex?q_%5Cast" alt> as a neural network (instead of a table).</p>
<p>Unfortunately, reinforcement learning is  <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.3097&amp;rep=rep1&amp;type=pdf">notoriously unstable</a>  when neural networks are used to represent the action values. In this lesson, you&apos;ll learn all about the Deep Q-Learning algorithm, which addressed these instabilities by using  <strong>two key features</strong>:</p>
<ul>
<li>Experience Replay</li>
<li>Fixed Q-Targets</li>
</ul>
<p>The <strong>Deep Q-Network (DQN) algorithm</strong> has caused a lot of buzz around deep RL since 2013. It&apos;s more or less an online version of a neural fitted value iteration paper from 2005 by Reed Miller and Martin, which introduced training of Q value function represented by a multilayer perceptron.</p>
<p>There are a few very useful additions and tweaks though in DQN.</p>
<ul>
<li>
<p>The first addition is the use of a rolling history of the past data via <strong>replay pool</strong>. By using the replay pool, the behavior distribution is averaged over many of its previous states, smoothing out learning and avoiding oscillations. The has the <strong>advantage</strong> that each step of the experience is potentially used in many weight updates.</p>
</li>
<li>
<p>The other big idea is the use of a target network to represent the old Q function, which will be used to compute the loss of every action during training. <strong>Why not use a single network?</strong> Well, the issue is that at each step of training, the Q function&apos;s values change and then the value estimates can easily spiral out of control.</p>
</li>
</ul>
<p>These additions enable RL agents to <strong>converge more reliably during training</strong>.</p>
<p>See the video <a href="https://youtu.be/WgiAvr7COR0">here</a>.</p>
<h2 class="mume-header" id="-additional-references">Additional References</h2>

<ul>
<li>
<p>Riedmiller, Martin. &quot;Neural fitted Q iteration&#x2013;first experiences with a data efficient neural reinforcement learning method.&quot; European Conference on Machine Learning. Springer, Berlin, Heidelberg, 2005.  <a href="http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf">http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf</a></p>
</li>
<li>
<p>Mnih, Volodymyr, et al. &quot;Human-level control through deep reinforcement learning.&quot; Nature518.7540 (2015): 529.  <a href="http://www.davidqiu.com:8888/research/nature14236.pdf">http://www.davidqiu.com:8888/research/nature14236.pdf</a></p>
</li>
</ul>
<h1 class="mume-header" id="-deep-q-network">Deep Q-Network</h1>

<p>In the video below, Arpan will tell you all about how DeepMind leveraged a <strong>Deep Q-Network (DQN)</strong> to build the Deep Q-Learning algorithm that learned to play many Atari video games better than humans.</p>
<p>In 2015, DeepMind made a breakthrough by designing an agent that learned to play video games better than humans. It&apos;s probably easy to write a program that plays pong perfectly if you have access to the underlying game state, position of the ball, paddles, etc. But this agent was only given raw pixel data, what a human player would see on screen. It learned to play a bunch of different Atari games, all from scratch. They called this agent a <strong>Deep Q Network</strong>. Let&apos;s see how it works.</p>
<p>True to its name, at the heart of the agent is a deep neural network that acts as a function approximator. You pass in images from your favorite video game one screen at a time, and it produces a vector of action values, with the max value indicating the action to take.</p>
<p>As a RL signal, it is fed back the change in game score at each time step. In the beginning when the neural network is initialized with random values, the actions taken are all over the place.</p>
<p>Over time it begins to associate situations and sequences in the game with appropriate actions and learns to actually play the game well.</p>
<p>Consider how complex the input space is. Atari games are displayed at a resolution of 210x160 pixles, with 128 possible colors for each pixel. <strong>This is still technically a discrete state space but very large to process as is</strong>.</p>
<p><strong>To reduce complexity,</strong> the DeepMind team decided to perform some minimal processing, convert the frames to grayscale, and scale them down to a square 84x84 pixel block. Square images allowed them to use more optimized neural network operations on GPUs.</p>
<p>In order to give agent access to a sequence of frames, they stacked four such frames together resulting in a final state space size of 84x84x4.</p>
<p>There might be other approaches to dealing with sequential data but this was a simple approach that seemed to work pretty well.</p>
<p>On the output side, unlike a traditional RL setup where only one Q value is produced at a time, the Deep Q Network is designed to produce a Q value for every possible action in a single forward pass. Without this, you would have to run the network individually for every action. Instead, you can now simply use this vector to take an action, either stochastically, or by choosing the one with the maximum value.</p>
<p>These innovative input and output transformations support a powerful yet simple neural network architecture undeer the hood. The screen images are first processed by convolutional layers. This allows the system to exploit spatial relationships, and can exploit spatial rule space.</p>
<p>Also, since four frames are stacked and provided as input, these convolutional layers also extract some temporal properties across those frames.</p>
<p>The original DQN agent used three such convolutional layers with relu activation. There were followed by one fully-connected hidden layer with relu activation, and one fully connected linear output that produced the vector of action values.</p>
<p align="center">
<img src="img/dqn1.png" alt="drawing" width="700">
</p>
<p>This same architecture was used for all Atari games they tested on, but each game was learned from scratch with a freshly initialized network.</p>
<p>Training such a network requires a lot of data, but even then, it is not guaranteed to converge on the optimal value function. In fact, there are situations where the network weights can oscillate or diverge, due to the high correlation between actions and states. This can result in a very unstable and ineffective policy.</p>
<p>In order to overcome these challenges, the researchers came up with several techniques that slightly modified the base Q-learning algorithm. We&apos;ll take a look at two of those techniques (which are the most important ones):</p>
<ul>
<li><strong>Experience Replay</strong></li>
<li><strong>Fixed Q Targets</strong></li>
</ul>
<p>See the video <a href="https://youtu.be/GgtR_d1OB-M">here</a>.</p>
<h1 class="mume-header" id="-experience-replay">Experience Replay</h1>

<p>The idea of experience replay and its application to training neural networks for RL isn&apos;t new. It was originally proposed to make more efficient use of observed experiences. Consider the basic online Q-learning algorithm where we interact with the environment and at each time step, we obtain a tuple like this: <img src="https://latex.codecogs.com/gif.latex?%5Cleft%20%5Clangle%20S_t%2C%20A_t%2C%20R_%7Bt+1%7D%2C%20S_%7Bt+1%7D%20%5Cright%20%5Crangle" alt>, learn from it and then discarded, moving on to the next tuple in the following timestep.</p>
<p>This seems a little wasteful. We could possibly learn more from these experienced tuples if we stored them somewhere.</p>
<p>Moreover, some states are pretty rare to come by and some actions can be pretty costly. So, it&apos;d be nice to recall such experiences.</p>
<p>That is exactly what a <strong>replay buffer</strong> allows us to do. We store each experienced tuple in this buffer as we are interacting with the environment and then sample a small batch of tuples from it in order to learn.</p>
<p>As a result, we are able to learn from individual tuples multiple times, recall rare occurences, and in general make better use of our experience.</p>
<p>But there is another critical problem that experience replay can help with and this is what DQN takes advantage of. If you think about the experiences being obtained, you realize that every action <img src="https://latex.codecogs.com/gif.latex?A_t" alt> affects the next state <img src="https://latex.codecogs.com/gif.latex?S_t" alt> in some way, which means that a sequence of experienced tuples can be highly correlated.</p>
<p>A naive Q-learning approach that learns from each of these experiences in sequential order runs the risk of getting swayed by the effects of this correlation.</p>
<p>With <strong>experience replay</strong>, we can sample from this buffer at random. It doesn&apos;t have to be in the same sequence as we stored the tuples. This helps break the correlation and ultimately prevents action values from oscillating or diverging catastrophically.</p>
<p>Let&apos;s look at an example for better understanding (<a href="https://youtu.be/wX_-SZG-YMQ">this video</a> from minute 2:00).</p>
<p>If you think about it, this approach is basically building a database of samples and then learning a mapping from them. In that sense, <strong>experience replay helps us reduce the RL problem or at least the value learning portion of it to a supervised learning scenario</strong>. We can then apply other models learning techniques and best practices developed in the supervised learning literature through the RL. We can even improve upon this idea, for example, by prioritizing experience tuples that are rare or more important.</p>
<p>See the video <a href="https://youtu.be/wX_-SZG-YMQ">here</a>.</p>
<h2 class="mume-header" id="-summary">Summary</h2>

<p>When the agent interacts with the environment, the sequence of experience tuples can be highly correlated. The naive Q-learning algorithm that learns from each of these experience tuples in sequential order runs the risk of getting swayed by the effects of this correlation. By instead keeping track of a  <strong>replay buffer</strong>  and using  <strong>experience replay</strong>  to sample from the buffer at random, we can prevent action values from oscillating or diverging catastrophically.</p>
<p>The  <strong>replay buffer</strong>  contains a collection of experience tuples <img src="https://latex.codecogs.com/gif.latex?%28S%2C%20A%2C%20R%2C%20%7BS%7D%27%29" alt>. The tuples are gradually added to the buffer as we are interacting with the environment.</p>
<p>The act of sampling a small batch of tuples from the replay buffer in order to learn is known as  <strong>experience replay</strong>. In addition to breaking harmful correlations, experience replay allows us to learn more from individual tuples multiple times, recall rare occurrences, and in general make better use of our experience.</p>
<h1 class="mume-header" id="-fixed-q-targets">Fixed Q-Targets</h1>

<p>Experience replay helps us address one type of correlation, i.e. between <strong>consecutive experience tuples</strong>.</p>
<p>There is another kind of correlation that Q-learning is susceptible to. Q-learning is a form of <strong>Temporal Difference (TD)</strong> learning. In the equation below, the first part is called <strong>TD target</strong>,</p>
<p align="center">
<img src="img/tempdiff1.png" alt="drawing" width="600">
</p>
<p>Our goal is to reduce the difference between this target and the currently predicted Q-value. This difference is the <strong>TD error</strong>.</p>
<p>Now, the <em>TD target</em> here is supposed to be a replacement for the true value function <img src="https://latex.codecogs.com/gif.latex?q_%5Cpi%28S%2CA%29" alt>, which is unknown to us.</p>
<p>We originally used <img src="https://latex.codecogs.com/gif.latex?q_%5Cpi" alt> to define a squared error loss, and differentiated that with respect to <img src="https://latex.codecogs.com/gif.latex?w" alt> to get our gradient descent update rule.</p>
<p>Now, <img src="https://latex.codecogs.com/gif.latex?q_%5Cpi" alt> is not dependent on our function approximation or its parameters, thus resulting in a simple derivative, and update rule.</p>
<p>But our TD target is dependent on these parameters which means simply replacing the true value function <img src="https://latex.codecogs.com/gif.latex?q_%5Cpi" alt> with a target like this is <em><strong>mathematically incorrect</strong></em> (see below).</p>
<p align="center">
<img src="img/tempdiff2.png" alt="drawing" width="500">
</p>
<p>We can get away with it in practice because every update results in a small change to the parameters. We&apos;re just generally in the right direction. If we set <img src="https://latex.codecogs.com/gif.latex?%5Calpha" alt> equals to 1, and leap toward the target then we&apos;d likely overshoot and land in the wrong place. Also, this is less of a concern when we use a lookup table or a dictionary since Q-values are stored separately for each state action pair. But it can affect learning significantly when we use function approximation, where all the Q-values are intrinsically tied together through the function parameters.</p>
<p>You maybe thinking, <strong>&quot;Doesn&apos;t experience replay take care of this problem?&quot;</strong>.</p>
<p>Well, it addresses a similar but slightly different issue. There we broke the correlation effects between consecutive experience tuples by sampling them randomly out of order. <strong>Here, the correlation is between the target and the parameters we are changing.</strong> This is like chasing a moving target, literally. In fact, it&apos;s worse. it&apos;s like trying to train a donkey to walk straight by sitting on it and dangling a carrot in front. Yes, the donkey might step forward and the carriage usually gets carried away always staying a little out of reach. But, contrary to popular belief, this doesn&apos;t quite work as you&apos;d expect. The carrot is much more likely to bounce around randomly throwing the donkey off with every jerky step. Each action affects the next position of the target in a very complicated and unpredictable manner. You shouldn&apos;t be surprised if the donkey gets frustrated jumping around the spot and gives up. Instead, you should get off the donkey stand in one place and dangle the carrot from there. Once the donkey reaches that spot, move a few steps ahead, dangle another carrot and repeat. What you&apos;re essentially doing is decoupling the target&apos;s position from the donkey&apos;s actions giving it a more stable learning environment.</p>
<p>We can do pretty much the same thing in Q-learning by fixing the function parameters used to generate our target. The fixed parameters indicated by a <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w%5E-" alt> (in equation below) are basically a copy of <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w" alt> that we don&apos;t change during the learning step.</p>
<p align="center">
<img src="img/tempdiff3.png" alt="drawing" width="500">
</p>
<p>In practice, we copied <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w" alt> into <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w%5E-" alt>, use it to generate targets while changing <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w" alt> for a certain number of learning steps. Then, we update <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w%5E-" alt> with the latest <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w" alt>, again learn from a number of steps and so on. This decouples the target from the parameters, makes the learning algorithm much more stable, and less likely to diverge or fall into oscillations.</p>
<p>See the video <a href="https://youtu.be/SWpyiEezfp4">here</a>.</p>
<h2 class="mume-header" id="-summary-1">Summary</h2>

<p>In Q-Learning, we  <strong><em>update a guess with a guess</em></strong>, and this can potentially lead to harmful correlations. To avoid this, we can update the parameters  ww  in the network <img src="https://latex.codecogs.com/gif.latex?%5Chat%7Bq%7D" alt> to better approximate the action value corresponding to state <img src="https://latex.codecogs.com/gif.latex?S" alt> and action <img src="https://latex.codecogs.com/gif.latex?A" alt> with the following update rule:</p>
<p align="center">
<img src="img/tempdiff1.png" alt="drawing" width="500">
</p>
<p>where <img src="https://latex.codecogs.com/gif.latex?w%5E-" alt> are the weights of a separate target network that are not changed during the learning step, and <img src="https://latex.codecogs.com/gif.latex?%28S%2C%20A%2C%20R%2C%20%7BS%7D%27%29" alt> is an experience tuple.</p>
<p><strong>Note</strong>: Ever wondered how the example in the video would look in real life? See: <a href="https://www.youtube.com/watch?v=-PVFBGN_zoM">Carrot Stick Riding</a>.</p>
<h1 class="mume-header" id="-deep-q-learning-algorithm">Deep Q-Learning Algorithm</h1>

<p>Please take the time now to read the  <a href="docs/DQNNaturePaper.pdf">research paper</a>  that introduces the Deep Q-Learning algorithm.</p>
<p align="center">
<img src="img/dqnalgo1.png" alt="drawing" width="600">
</p>
<h2 class="mume-header" id="-reading-scientific-papers">Reading Scientific Papers</h2>

<p>As part of this nanodegree, you will learn about many of the most recent, cutting-edge algorithms! Because of this, it will prove useful to learn how to read the original research papers. Here are some  <a href="https://www.huffingtonpost.com/jennifer-raff/how-to-read-and-understand-a-scientific-paper_b_5501628.html">excellent tips</a>. Some of the best advice is:</p>
<ul>
<li>
<p>Take notes.</p>
</li>
<li>
<p>Read the paper multiple times. On the first couple readings, try to focus on the main points:</p>
<ol>
<li>What kind of tasks are the authors using deep reinforcement learning (RL) to solve? What are the states, actions, and rewards?</li>
<li>What neural network architecture is used to approximate the action-value function?</li>
<li>How are experience replay and fixed Q-targets used to stabilize the learning algorithm?</li>
<li>What are the results?</li>
</ol>
</li>
<li>
<p>Understanding the paper will probably take you longer than you think. Be patient, and reach out to the Udacity community with any questions.</p>
</li>
</ul>
<h2 class="mume-header" id="-check-your-understanding">Check Your Understanding</h2>

<p>After you have read the paper, use <a href="https://youtu.be/MqTXoCxQ_eY">the video below</a> to check your understanding.</p>
<p><em>from the video</em></p>
<p>There are <strong>two main processes</strong> that are interleaved in this algorithm.</p>
<ul>
<li>One is where we sample the the environment by performing actions and store away the observed experienced tuples in a replay memory.</li>
<li>The other is where we select all the small batch of tuples from this memory randomly and learn from the batch using a gradient descent update step.</li>
</ul>
<p align="center">
<img src="img/dqnalgo2.png" alt="drawing" width="600">
</p>
<p>These two processes are not directly dependent on each other. So, you could perform multiple sampling steps, then one learning step, or even multiple learning steps with different random batches.</p>
<p>The rest of the algorithm is designed to support these steps.</p>
<p>In the beginning you need to initialize an empty replay memory <img src="https://latex.codecogs.com/gif.latex?D" alt>. Note that memory is finite, so you may want to use something like a circular Q that retains the <img src="https://latex.codecogs.com/gif.latex?N" alt> most recent experience tuples.</p>
<p>Then, you also need to initialize the parameters or weights of your neural network. There are certain best practices that you can use, for instance, sampel the weights randomly from a normal distribution with variance equal to 2 by the number of inputs to each neuron. These initialization methods are typically available in modern deep learning libraries like Keras and Tensorflow, so you won&apos;t need to implement them yourself.</p>
<p>To use the fixed Q-targets technique, you need a second set of parameters <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w%5E-" alt> which you can initialize to <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w" alt>.</p>
<p align="center">
<img src="img/dqnalgo3.png" alt="drawing" width="600">
</p>
<p>Now, remember that the specific algorithm was designed to work with video games. So, for each episode and each timestep <img src="https://latex.codecogs.com/gif.latex?t" alt> within that episode you observe a raw screen image or input frame <img src="https://latex.codecogs.com/gif.latex?x_t" alt> which you need to convert to grayscale and crop to a square size, etc.</p>
<p>Also, in order to capture temporal relationships you can stack a few input frames to biuld each state vector. Let&apos;s denote this pre-processing and stacking operation by the function <img src="https://latex.codecogs.com/gif.latex?%5Cphi" alt>, which takes a sequence of frames and produces some combined representation.</p>
<p>Note that if we want to stack say four frames, we&apos;ll have to do something special for the first three timesteps. For instance, we can treat those missing frames as blank, or just use copies of the first frame, or we can just skip storing the experience tuples till we get a complete sequence.</p>
<p>In practice, you won&apos;t be able to run the learning step immediately. You&apos;ll need to wait till you have sufficient number of tuples in memory. Note that we do not clear out the memory after each episode, this enables us to recall and build batches of experiences from across episodes.</p>
<p>There are many other techniques and optimizations that are used in the DQN paper, such as reward clipping, error clipping, storing past actions as part of the state vector, dealing with terminal states, decaying epsilon over time, etc.</p>
<p>See the video <a href="https://youtu.be/MqTXoCxQ_eY">here</a>.</p>
<h1 class="mume-header" id="-coding-exercise">Coding Exercise</h1>

<p>In this exercise, you will implement Deep Q-Learning to solve OpenAI Gym&apos;s LunarLander environment. To begin, open the Workspace in the next concept, navigate to the  <code>codes/</code>  folder, and follow the instructions in  <code>Deep_Q_Network.ipynb</code>.</p>
<p>After you are able to get the code working, try to change the hyperparameters and architecture, to see if you can get the agent to train faster!</p>
<h2 class="mume-header" id="-pytorch">PyTorch</h2>

<p>The implementation is written in PyTorch. If the PyTorch framework is new to you, please take a look at our  <strong>Deep Learning with PyTorch</strong>  lesson in the extracurricular content.</p>
<h2 class="mume-header" id="-note">Note</h2>

<p>In the Workspace in the following concept, you will have the option to  <strong>ENABLE GPU</strong>  to accelerate training. After training, you can use the provided code in the Jupyter notebook to watch your agent&apos;s performance. Note that if visualizing the trained agent in the Workspace, GPU should be  <strong>disabled</strong>  -- otherwise the notebook will return an error.</p>
<p>Thus, you are encouraged to follow the following workflow:</p>
<ol>
<li>train the agent with GPU  <strong>enabled</strong>, and save the trained model weights,</li>
<li><strong>disable GPU</strong>, load the trained weights from file, and watch the trained agent.</li>
</ol>
<h2 class="mume-header" id="-dqn-improvements">DQN Improvements</h2>

<p>Later in this lesson, you will learn about three different improvements you can make to your algorithm:</p>
<ul>
<li>Double DQN</li>
<li>Prioritized Experience Replay</li>
<li>Dueling DQN</li>
</ul>
<p>If you&apos;d like to implement any of these, you&apos;re encouraged to use the provided notebook as a starting point.</p>
<h1 class="mume-header" id="-deep-q-learning-improvements">Deep Q-Learning Improvements</h1>

<p>Several improvements to the original Deep Q-Learning algorithm have been suggested. Over the next several videos, we&apos;ll look at three of the more prominent ones.</p>
<p align="center">
<img src="img/dueling-q-network.png" alt="drawing" width="600">
</p>
<h2 class="mume-header" id="-double-dqn">Double DQN</h2>

<p>Deep Q-Learning  <a href="https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf">tends to overestimate</a>  action values.  <a href="https://arxiv.org/abs/1509.06461">Double Q-Learning</a>  has been shown to work well in practice to help with this.</p>
<h2 class="mume-header" id="-prioritized-experience-replay">Prioritized Experience Replay</h2>

<p>Deep Q-Learning samples experience transitions  <em>uniformly</em>  from a replay memory.  <a href="https://arxiv.org/abs/1511.05952">Prioritized experienced replay</a>  is based on the idea that the agent can learn more effectively from some transitions than from others, and the more important transitions should be sampled with higher probability.</p>
<h2 class="mume-header" id="-dueling-dqn">Dueling DQN</h2>

<p>Currently, in order to determine which states are (or are not) valuable, we have to estimate the corresponding action values  <em>for each action</em>. However, by replacing the traditional Deep Q-Network (DQN) architecture with a  <a href="https://arxiv.org/abs/1511.06581">dueling architecture</a>, we can assess the value of each state, without having to learn the effect of each action.</p>
<h1 class="mume-header" id="-double-dqn-1">Double DQN</h1>

<p>The first problem we&apos;re going to address is the <strong>overestimaton</strong> of action values that Q-learning is prone to.</p>
<p>Let&apos;s look back at the update rule for Q-learning with function approximation and focus on the TD target.</p>
<p align="center">
<img src="img/tempdiff1.png" alt="drawing" width="500">
</p>
<p>Here, the max operation is necessary to find the best possible value we could get from the next state. To understand this better, let&apos;s rewrite the target, and expand the max oeprartion.</p>
<p align="center">
<img src="img/ddqn1.png" alt="drawing" width="500">
</p>
<p>It&apos;s just a more efficient way of saying, that we want to obtain the Q-value for state <img src="https://latex.codecogs.com/gif.latex?%7BS%7D%27" alt> and the action that results in the maximum Q-value among all possible actions from that state.</p>
<p>When we write it this way, we can see that it&apos;s possible for the argmax operation to make a mistake, especially in the early stages. <strong>Why?</strong></p>
<p>Because the Q-values are still evolving, and we may not have gathered enough information to figure out the best action. The accuracy of our Q-values depends a lot on what actions have been tried, and what neighboring states have been explored.</p>
<p>In fact, it has been shown that this results in an overestimation of Q-values, since we always pick the maximum among a set of noisy numbers. So, maybe we shouldn&apos;t blindly trust these values. <strong>What can we do to make our estimation more robust?</strong></p>
<p>One idea that has been shown to work very well in practice is called <strong>Double Q-learning</strong>, where we select the best action using one set of parameters <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w" alt>, but evaluate is using a different set of parameters <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w&apos;" alt>. It&apos;s basically like having two separate function approximators that must agree on the best action. If <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w" alt> picks an action that is not the best according to <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w&apos;" alt>, then the Q-value returned is not that high.</p>
<p align="center">
<img src="img/ddqn2.png" alt="drawing" width="500">
</p>
<p>In the long run, this prevents the algorithm from propagating incidental high rewards that may have been obtained by chance, and don&apos;t reflect long-term returns.</p>
<p>Now you might be thinking <strong>where do we get the second set of parameters from?</strong></p>
<p>In the original formulation of Double Q-learning, you would basically maintain two value functions and randomly choose one of them to update at each step using the other only for evaluating actions.</p>
<p>But when using DQN with fixed Q-targets, we already have an alternate set of parameters. Remember <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w%5E-" alt>?</p>
<p>It turns out that since <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w%5E-" alt> is kept frozen for a while, it is different from <img src="https://latex.codecogs.com/gif.latex?%5Cbold%20w" alt> that it can be reused for this purpose.</p>
<p>And that&apos;s it! This simple modification keeps Q-values in check, preventing them from exploding in early stages of learning or fluctuating later on. The resulting policies have also been shown to perform significantly better than vanilla DQNs.</p>
<p>See the video <a href="https://youtu.be/PGCEMLujiGI">here</a>.</p>
<h2 class="mume-header" id="-notes">Notes</h2>

<p>You can read more about Double DQN (DDQN) by perusing this  <a href="https://arxiv.org/abs/1509.06461">research paper</a>.</p>
<p>If you&apos;d like to dig deeper into how Deep Q-Learning overestimates action values, please read this  <a href="https://www.ri.cmu.edu/pub_files/pub1/thrun_sebastian_1993_1/thrun_sebastian_1993_1.pdf">research paper</a>.</p>
<h1 class="mume-header" id="-prioritized-experience-replay-1">Prioritized Experience Replay</h1>

<p>The next issue we&apos;ll look at is related to experience replay. Recall the basic idea behind it. We interact with the environment to collect experience tuples, save them in a buffer, and then later, we randomly sample a batch to learn from. This helps us break the correlation between consecutive experiences and stabilizes our learning algorithm.</p>
<p>But some of these experiences may be more important for learning than others. Moreover, these important experiences might occur infrequently. If we sample the batches uniformly, then these experiences have a very small chance of getting selected.</p>
<p>Since buffers are practically limited in capacity, older important experiences may get lost. This is where the idea of <strong>prioritized experience replay</strong> comes in. <strong>But what criteria should we use to assign priorities to each tuple?</strong></p>
<p>One approach is to use the <em><strong>TD error delta</strong></em>: <em>the bigger the error, the more we expect to learn from that tuple.</em> So, let&apos;s take the magnitude of this error as a measure of priority and store it along with each corresponding tuple in the replay buffer.</p>
<p>When creating batches, we can use this value to compute a sampling probability. Select any tuple <img src="https://latex.codecogs.com/gif.latex?i" alt> with a probability equal to its priority value <img src="https://latex.codecogs.com/gif.latex?P%28i%29" alt>, normilized by the sum of all priority values in the replay buffer.</p>
<p align="center">
<img src="img/per1.png" alt="drawing" width="500">
</p>
<p>When a tuple is picked, we can update its priority with a newly computed TD error using the latest <img src="https://latex.codecogs.com/gif.latex?q" alt> values. This seems to work fairly well and has been shown to reduce the number of batch updates needed to learn a value function.</p>
<p>There are couple of things we can improve.</p>
<ul>
<li>
<p>First, note that if the TD error is zero, then the priority value of the tuple and hence its probability of being picked will also be zero. Zero or very low TD error doesn&apos;t necessarily mean we have nothing more to learn from such a tuple, it might be the case that our estimate was closed due to the limited samples we visited till that point. So, to prevent such tuples from being starved for selection, we can add a small constant <img src="https://latex.codecogs.com/gif.latex?e" alt> to every priority value.</p>
</li>
<li>
<p>Another issue along similar line is that greedily using these priority values may lead to a small subset of experiences being replayed over and over resulting in a overfitting to that subset. To avoid this, we can reintroduce some element of uniform random sampling. This adds another hyperparameter <img src="https://latex.codecogs.com/gif.latex?A" alt> which we use to redefine the sampling probability as <img src="https://latex.codecogs.com/gif.latex?P%28i%29%3D%5Cfrac%7Bp_%7Bi%7D%5E%7Ba%7D%7D%7B%5Csum%20p_%7Bk%7D%5E%7Ba%7D%7D" alt>. We can control how much we want to use priorities versus randomness by varying this parameter, where <img src="https://latex.codecogs.com/gif.latex?a%3D0" alt> corresponds to pure uniform randomness and <img src="https://latex.codecogs.com/gif.latex?a%3D1" alt> only uses priorities.</p>
</li>
<li>
<p>When we use prioritized experience replay, we have ot make one adjustment to our update rule. Remember that our original Q-learning update is derived from an experience over all experiences. When using a stochastic update rule, the way we sample these experiences must match the underlying distribution they came from. This is preserved when we sample experience tuples uniformly from the replay buffer, but this assumption is violated when we use a non-uniform sampling, for example, using priorities. The Q values we learn will be biased according to these priority values which we only wanted to use for sampling. <em><strong>To correct for this bias</strong></em>, we need to introduce an importance-sampling weight equal to <img src="https://latex.codecogs.com/gif.latex?%5Cinline%20%5Cfrac%7B1%7D%7BN%7D%5Cfrac%7B1%7D%7BP%28i%29%7D" alt>, where <img src="https://latex.codecogs.com/gif.latex?N" alt> is the size of this replay buffer. We can add another hyperparameter <img src="https://latex.codecogs.com/gif.latex?b" alt> and raise each importance-sampling weight to <img src="https://latex.codecogs.com/gif.latex?b" alt>, to control how much these weights affect learning. In fact, these weights are more important toward the end of learning when your Q values begin to converge. So, you can increase <img src="https://latex.codecogs.com/gif.latex?b" alt> from a low value to 1 over time.</p>
</li>
</ul>
<p align="center">
<img src="img/per2.png" alt="drawing" width="600">
</p>
<p>See the video <a href="https://youtu.be/cN8z-7Ze9L8">here</a>.</p>
<p><strong>NOTE:</strong> You can read more about prioritized experience replay by perusing this <a href="https://arxiv.org/abs/1511.05952">research paper</a>.</p>
<h1 class="mume-header" id="-dueling-dqn-1">Dueling DQN</h1>

<p>The final enhancement of DQNs that we will briefly look at is appropriately titled <strong>Dueling Networks.</strong> Below is a typical DQN architecture. A sequence of convolutional layers followed by a couple of fully connected layers that produce Q values. The <strong>core idea</strong> of dueling networks is to use two streams, one that estimates the state value function, and one that estimates the advantage for each action.</p>
<p align="center">
<img src="img/duel1.png" alt="drawing" width="600">
</p>
<p>These streams may share some layers in the beginning such as convolutional layers, then branch off with their own fully-connected layers. Finally, the desired Q values are obtained by combining the state and advantage values. The intuition behind this is that the value of most states don&apos;t vary a lot across actions. So, it makes sense to try and directly estimate them, but we still need to capture the difference actions make in each state.</p>
<p>This is where the advantage function comes in. Some modifications are necessary to adapt Q learning to this architecture, which you can find in the dueling network&apos;s paper. Along with double DQN and prioritized replay, this technique has resulted in significant improvement over vanilla DQNs.</p>
<p>See the video <a href="https://youtu.be/zZeHbPs39Ls">here</a>.</p>
<p><strong>NOTE:</strong> You can read more about Dueling DQN by perusing this <a href="https://arxiv.org/abs/1511.06581">research paper</a>.</p>
<h1 class="mume-header" id="-rainbow">Rainbow</h1>

<p>So far, you&apos;ve learned about three extensions to the Deep Q-Networks (DQN) algorithm:</p>
<ul>
<li>Double DQN (DDQN)</li>
<li>Prioritized experience replay</li>
<li>Dueling DQN</li>
</ul>
<p>But these aren&apos;t the only extensions to the DQN algorithm! Many more extensions have been proposed, including:</p>
<ul>
<li>Learning from  <a href="https://arxiv.org/abs/1602.01783">multi-step bootstrap targets</a>  (as in A3C -  <em>you&apos;ll learn about this in the next part of the nanodegree</em>)</li>
<li><a href="https://arxiv.org/abs/1707.06887">Distributional DQN</a></li>
<li><a href="https://arxiv.org/abs/1706.10295">Noisy DQN</a></li>
</ul>
<p>Each of the six extensions address a  <strong><em>different</em></strong>  issue with the original DQN algorithm.</p>
<p>Researchers at Google DeepMind recently tested the performance of an agent that incorporated all six of these modifications. The corresponding algorithm was termed  <a href="https://arxiv.org/abs/1710.02298">Rainbow</a>.</p>
<p>It outperforms each of the individual modifications and achieves state-of-the-art performance on Atari 2600 games!</p>
<p align="center">
<img src="img/rainbow.png" alt="drawing" width="500">
</p>
<h2 class="mume-header" id="-in-practice">In Practice</h2>

<p>In mid-2018, OpenAI held  <a href="https://contest.openai.com/">a contest</a>, where participants were tasked to create an algorithm that could learn to play the  <a href="https://en.wikipedia.org/wiki/Sonic_the_Hedgehog">Sonic the Hedgehog</a>  game. The participants were tasked to train their RL algorithms on provided game levels; then, the trained agents were ranked according to their performance on previously unseen levels.</p>
<p>Thus, the contest was designed to assess the ability of trained RL agents to generalize to new tasks.</p>
<p align="center">
<img src="img/sonic.gif" alt="drawing" width="500">
</p>
<p>One of the provided baseline algorithms was <strong>Rainbow DQN</strong>. If you&apos;d like to play with this dataset and run the baseline algorithms, you&apos;re encouraged to follow the <a href="https://contest.openai.com/2018-1/details/">setup instructions</a>.</p>
<p align="center">
<img src="img/rainbow2.png" alt="drawing" width="600">
</p>

      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>