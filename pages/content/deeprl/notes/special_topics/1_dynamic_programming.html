<!DOCTYPE html><html><head>
      <title>1_dynamic_programming</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:////Users/abakh005/.vscode/extensions/shd101wyy.markdown-preview-enhanced-0.6.7/node_modules/@shd101wyy/mume/dependencies/katex/katex.min.css">
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.markdown-preview .md-toc{padding:0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link p,.markdown-preview .md-toc .md-toc-link-wrapper .md-toc-link div{display:inline}.markdown-preview .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc{padding:0 16px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link{display:inline;padding:.25rem 0}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link p,html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper .md-toc-link div{display:inline}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc .md-toc .md-toc-link-wrapper.highlighted .md-toc-link{font-weight:800}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  300px/2);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <h1 class="mume-header" id="-introduction">Introduction</h1>

<p>For this lesson, we&apos;ll confine our attention to a problem that&apos;s slightly easier than the RL problem. Instead of working in a setting where the agent has to learn from interaction, we&apos;ll assume that the agent already knows everything about the environment.</p>
<p>So the agent knows how the environment decides the next state, and it knows how the environment decides reward. The goal will remain the same. Given this information, the agent would like to find the optimal policy.</p>
<p>Solving the simpler problem first will prove incredibly useful for building intuition before we tackle the full RL problem.</p>
<p>See the video <a href="https://youtu.be/ek2PD9RDrWw">here</a>.</p>
<p>In the  <strong>dynamic programming</strong>  setting, the agent has full knowledge of the Markov decision process (MDP) that characterizes the environment. (This is much easier than the  <strong>reinforcement learning</strong>  setting, where the agent initially knows nothing about how the environment decides state and reward and must learn entirely from interaction how to select actions.)</p>
<p>This lesson covers material in  <strong>Chapter 4</strong>  (especially 4.1-4.4) of the  <a href="http://go.udacity.com/rl-textbook">textbook</a>.</p>
<h1 class="mume-header" id="-openai-gym-frozenlakeenv">OpenAI Gym: FrozenLakeEnv</h1>

<p>In this lesson, you will write your own Python implementations of all of the algorithms that we discuss. While your algorithms will be designed to work with any OpenAI Gym environment, you will test your code with the FrozenLake environment.</p>
<p><a href="https://classroom.udacity.com/nanodegrees/nd893/parts/23d1307b-b908-436f-bdfe-78b6c5712b04/modules/9765795d-fea0-43f7-b49e-0bddba750950/lessons/62060219-95b0-4d08-8269-3b963c1c27bb/concepts/c0916676-e356-46df-9b22-cd63cc17e0a5#"></a></p>
<p><img src="https://video.udacity-data.com/topher/2017/September/59cc32c6_frozen-lake-6/frozen-lake-6.jpg" alt></p>
<p>Source:  <a href="http://eskipaper.com/images/frozen-lake-6.jpg">http://eskipaper.com/images/frozen-lake-6.jpg</a></p>
<p>In the FrozenLake environment, the agent navigates a 4x4 gridworld. You can read more about the environment in its corresponding  <a href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py">GitHub file</a>, by reading the commented block in the  <code>FrozenLakeEnv</code>  class. For clarity, we have also pasted the description of the environment below:</p>
<pre data-role="codeBlock" data-info class="language-"><code>    &quot;&quot;&quot;
    Winter is here. You and your friends were tossing around a frisbee at the park
    when you made a wild throw that left the frisbee out in the middle of the lake.
    The water is mostly frozen, but there are a few holes where the ice has melted.
    If you step into one of those holes, you&apos;ll fall into the freezing water.
    At this time, there&apos;s an international frisbee shortage, so it&apos;s absolutely imperative that
    you navigate across the lake and retrieve the disc.
    However, the ice is slippery, so you won&apos;t always move in the direction you intend.
    The surface is described using a grid like the following
        SFFF
        FHFH
        FFFH
        HFFG
    S : starting point, safe
    F : frozen surface, safe
    H : hole, fall to your doom
    G : goal, where the frisbee is located
    The episode ends when you reach the goal or fall in a hole.
    You receive a reward of 1 if you reach the goal, and zero otherwise.

    &quot;&quot;&quot;&quot;

</code></pre><h2 class="mume-header" id="-the-dynamic-programming-setting">The Dynamic Programming Setting</h2>

<p>Environments in OpenAI Gym are designed with the reinforcement learning setting in mind. For this reason, OpenAI Gym does not allow easy access to the underlying one-step dynamics of the Markov decision process (MDP).</p>
<p>Towards using the FrozenLake environment for the dynamic programming setting, we had to first download the  <a href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py">file</a>  containing the  <code>FrozenLakeEnv</code>  class. Then, we added a single line of code to share the one-step dynamics of the MDP with the agent.</p>
<pre data-role="codeBlock" data-info class="language-"><code># obtain one-step dynamics for dynamic programming setting
self.P = P

</code></pre><p>The new  <code>FrozenLakeEnv</code>  class was then saved in a Python file  <strong><a href="http://frozenlake.py">frozenlake.py</a></strong>, which we will use (instead of the original OpenAI Gym file) to create an instance of the environment.</p>
<h1 class="mume-header" id="-your-workspace">Your Workspace</h1>

<p>You will write all of your implementations within the classroom, using an interface identical to the one shown below. Your Workspace contains five files:</p>
<ul>
<li><strong><a href="http://frozenlake.py">frozenlake.py</a></strong>  - contains the  <code>FrozenLakeEnv</code>  class</li>
<li><strong>Dynamic_Programming.ipynb</strong>  - the mini project notebook where you will write all of your implementations (<em>this is the  <strong>only</strong>  file that you will modify!</em>)</li>
<li><strong>Dynamic_Programming_Solution.ipynb</strong>  - the instructor solutions corresponding to the mini project notebook</li>
<li><strong>check_test.py</strong>  - contains unit tests that you will use to verify that your implementations are correct</li>
<li><strong>plot_utils.py</strong>  - contains a plotting function for visualizing state-value functions</li>
</ul>
<p>The  <strong>Dynamic_Programming.ipynb</strong>  notebook can be found below.</p>
<blockquote>
<p>Note that it is broken into parts, which are designed to be completed at different parts of the lesson. For instance, you will complete Parts 0 and 1 in the concept titled  <strong>Mini Project: DP (Parts 0 and 1)</strong>. Then, you should wait to complete Part 2 until you reach the  <strong>Mini Project: DP (Part 2)</strong>  concept. DO NOT COMPLETE THE ENTIRE NOTEBOOK ALL AT ONCE! &#x1F603;</p>
</blockquote>
<p>To peruse the other files, you need only click on &quot;jupyter&quot; in the top left corner to return to the Notebook dashboard.</p>
<p><a href="https://classroom.udacity.com/nanodegrees/nd893/parts/23d1307b-b908-436f-bdfe-78b6c5712b04/modules/9765795d-fea0-43f7-b49e-0bddba750950/lessons/62060219-95b0-4d08-8269-3b963c1c27bb/concepts/dd004fdc-62cf-4a5f-9d9d-3b433681117a#"></a></p>
<p><img src="https://video.udacity-data.com/topher/2018/April/5ad2509c_screen-shot-2017-12-17-at-9.41.03-am/screen-shot-2017-12-17-at-9.41.03-am.png" alt></p>
<p>Please do not write or execute any code just yet. We&apos;ll get started with coding within the Workspace in a few concepts!</p>
<p>Review the codes under the <a href="codes/Dynamic_Programming_Solution.ipynb">codes folder</a>.</p>
<h1 class="mume-header" id="-another-grid-world-example">Another Grid World Example</h1>

<p>Watch the video <a href="https://youtu.be/n9SbomnLb-U">here</a>.</p>
<p>In this simple gridworld example, you may find it easy to determine the optimal policy by visual inspection. Of course, solving Markov decision processes (MDPs) corresponding to real world problems will prove far more challenging! &#x1F603;</p>
<p>To avoid over-complicating the theory, we&apos;ll use this simple example to illustrate the same algorithms that are used to solve much more complicated MDPs.</p>
<h1 class="mume-header" id="-an-iterative-method-part-1">An Iterative Method, Part 1</h1>

<p>Watch the video <a href="https://youtu.be/AX-hG3KvwzY">here</a>.</p>
<h1 class="mume-header" id="-an-iterative-method-part-2">An Iterative Method, Part 2</h1>

<p align="center">
<img src="img/st1.png" alt="drawing" width="750">
</p>
<p align="center">
<img src="img/st2.png" alt="drawing" width="750">
</p>
<p align="center">
<img src="img/st3.png" alt="drawing" width="750">
</p>
<p><strong>Note</strong>. This example serves to illustrate the fact that it is <strong><em>possible</em></strong> to <em>directly</em> solve the system of equations given by the Bellman expectation equation for v_\piv&#x3C0;&#x200B;. However, in practice, and especially for much larger Markov decision processes (MDPs), we will instead use an <em>iterative</em> solution approach.</p>
<h1 class="mume-header" id="-quiz-an-iterative-method">Quiz: An Iterative Method</h1>

<p align="center">
<img src="img/st4.png" alt="drawing" width="750">
</p>
<h2 class="mume-header" id="-an-iterative-method">An Iterative Method</h2>

<p align="center">
<img src="img/st5.png" alt="drawing" width="750">
</p>
<h1 class="mume-header" id="-iterative-policy-evaluation">Iterative Policy Evaluation</h1>

<p>Watch the video <a href="https://youtu.be/fVUpoyZDyGE">here</a>.</p>
<h1 class="mume-header" id="-implementation-iterative-policy-evaluation">Implementation: Iterative Policy Evaluation</h1>

<p align="center">
<img src="img/st6.png" alt="drawing" width="750">
</p>
<p>Please use the next concept to complete  <strong>Part 0: Explore FrozenLakeEnv</strong>  and  <strong>Part 1: Iterative Policy Evaluation</strong>  of  <code>Dynamic_Programming.ipynb</code>. Remember to save your work!</p>
<p>If you&apos;d like to reference the pseudocode while working on the notebook, you are encouraged to open  <a href="https://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf">this sheet</a>  in a new window.</p>
<p>Feel free to check your solution by looking at the corresponding sections in  <code>Dynamic_Programming_Solution.ipynb</code>. (<em>In order to access this file, you need only click on &quot;jupyter&quot; in the top left corner to return to the Notebook dashboard.</em>)</p>
<p><a href="https://classroom.udacity.com/nanodegrees/nd893/parts/23d1307b-b908-436f-bdfe-78b6c5712b04/modules/9765795d-fea0-43f7-b49e-0bddba750950/lessons/62060219-95b0-4d08-8269-3b963c1c27bb/concepts/90db191c-be3f-42d8-ac98-11fe6ad8d1de#"></a></p>
<p><img src="https://video.udacity-data.com/topher/2018/January/5a61eb50_screen-shot-2017-12-17-at-9.41.03-am/screen-shot-2017-12-17-at-9.41.03-am.png" alt></p>
<p>To find  <code>Dynamic_Programming_Solution.ipynb</code>, return to the Notebook dashboard.</p>
<h3 class="mume-header" id="-optional-additional-note-on-the-convergence-conditions">(Optional) Additional Note on the Convergence Conditions</h3>

<p>To see intuitively  <em>why</em>  the conditions for convergence make sense, consider the case that neither of the conditions are satisfied, so:</p>
<p align="center">
<img src="img/st7.png" alt="drawing" width="750">
</p>
<h1 class="mume-header" id="-mini-project-dp-part-0-and-1">Mini Project: DP (part 0 and 1)</h1>

<p>Refer to the <a href="codes/DP_mini_project_part1/Dynamic_Programming_Solution.ipynb">codes section here</a></p>
<h1 class="mume-header" id="-action-values">Action Values</h1>

<p align="center">
<img src="img/st8.png" alt="drawing" width="750">
</p>
<p align="center">
<img src="img/st9.png" alt="drawing" width="750">
</p>
<p align="center">
<img src="img/st10.png" alt="drawing" width="750">
</p>
<h1 class="mume-header" id="-implementation-estimation-of-action-values">Implementation: Estimation of Action Values</h1>

<p align="center">
<img src="img/st11.png" alt="drawing" width="750">
</p>
<p>If you&apos;d like to reference the pseudocode while working on the notebook, you are encouraged to open  <a href="https://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf">this sheet</a>  in a new window.</p>
<p>Feel free to check your solution by looking at the corresponding section in  <code>Dynamic_Programming_Solution.ipynb</code>.</p>
<h1 class="mume-header" id="-mini-project-dp-part-2">Mini Project: DP, Part 2</h1>

<p>See the codes <a href="codes/DP_mini_project_part2/Dynamic_Programming_Solution.ipynb">here</a></p>
<h1 class="mume-header" id="-policy-improvement">Policy Improvement</h1>

<p>Watch the video <a href="https://youtu.be/4_adUEK0IHg">here</a>.</p>
<h1 class="mume-header" id="-implementation-policy-improvement">Implementation: Policy Improvement</h1>

<p align="center">
<img src="img/st12.png" alt="drawing" width="750">
</p>
<p>Please use the next concept to complete  <strong>Part 3: Policy Improvement</strong>  of  <code>Dynamic_Programming.ipynb</code>. Remember to save your work!</p>
<p>If you&apos;d like to reference the pseudocode while working on the notebook, you are encouraged to open  <a href="https://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf">this sheet</a>  in a new window.</p>
<p>Feel free to check your solution by looking at the corresponding section in  <code>Dynamic_Programming_Solution.ipynb</code>.</p>
<h1 class="mume-header" id="-mini-project-dp-part-3">Mini Project: DP, Part 3</h1>

<p>See the codes <a href="codes/DP_mini_project_part3/Dynamic_Programming_Solution.ipynb">here</a></p>
<h1 class="mume-header" id="-policy-iteration">Policy Iteration</h1>

<p>Watch the video <a href="https://youtu.be/gqv7o1kBDc0">here</a>.</p>
<h1 class="mume-header" id="-implementation-policy-iteration">Implementation: Policy Iteration</h1>

<p align="center">
<img src="img/st13.png" alt="drawing" width="750">
</p>
<h1 class="mume-header" id="-mini-project-dp-part-4">Mini Project: DP, Part 4</h1>

<p>See the codes <a href="codes/DP_mini_project_part4/Dynamic_Programming_Solution.ipynb">here</a></p>
<h1 class="mume-header" id="-truncated-policy-iteration">Truncated Policy Iteration</h1>

<p>Watch the video <a href="https://youtu.be/a-RvCxlPMho">here</a>.</p>
<h1 class="mume-header" id="-implementation-truncated-policy-iteration">Implementation: Truncated Policy Iteration</h1>

<p>In the previous concept, you learned about  <strong>truncated policy evaluation</strong>. Whereas (iterative) policy evaluation applies as many Bellman updates as needed to attain convergence, truncated policy evaluation only performs a fixed number of sweeps through the state space.</p>
<p>The pseudocode can be found below.</p>
<p align="center">
<img src="img/st14.png" alt="drawing" width="750">
</p>
<p>We can incorporate this amended policy evaluation algorithm into an algorithm similar to policy iteration, called  <strong>truncated policy iteration</strong>.</p>
<p>The pseudocode can be found below.</p>
<p align="center">
<img src="img/st15.png" alt="drawing" width="750">
</p>
<p>You may also notice that the stopping criterion for truncated policy iteration differs from that of policy iteration. In policy iteration, we terminated the loop when the policy was unchanged after a single policy improvement step. In truncated policy iteration, we stop the loop only when the value function estimate has converged.</p>
<p>You are strongly encouraged to try out both stopping criteria, to build your intuition. However, we note that checking for an unchanged policy is unlikely to work if the hyperparameter  <code>max_iterations</code>  is set too small. (To see this, consider the case that  <code>max_iterations</code>  is set to a small value. Then even if the algorithm is far from convergence to the optimal value function  v_*v&#x2217;&#x200B;  or optimal policy  \pi_*&#x3C0;&#x2217;&#x200B;, you can imagine that updates to the value function estimate  VV  may be too small to result in any updates to its corresponding policy.)</p>
<p>Please use the next concept to complete  <strong>Part 5: Truncated Policy Iteration</strong>  of  <code>Dynamic_Programming.ipynb</code>. Remember to save your work!</p>
<p>If you&apos;d like to reference the pseudocode while working on the notebook, you are encouraged to open  <a href="https://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf">this sheet</a>  in a new window.</p>
<p>Feel free to check your solution by looking at the corresponding section in  <code>Dynamic_Programming_Solution.ipynb</code>.</p>
<h1 class="mume-header" id="-mini-project-dp-part-5">Mini Project: DP, Part 5</h1>

<p>See the codes <a href="codes/DP_mini_project_part5/Dynamic_Programming_Solution.ipynb">here</a></p>
<h1 class="mume-header" id="-value-iteration">Value Iteration</h1>

<p>Watch the video <a href="https://youtu.be/XNeQn8N36y8">here</a>.</p>
<h1 class="mume-header" id="-implementation-value-iteration">Implementation: Value Iteration</h1>

<p align="center">
<img src="img/st16.png" alt="drawing" width="750">
</p>
<p align="center">
<img src="img/st17.png" alt="drawing" width="750">
</p>
<p>Please use the next concept to complete  <strong>Part 6: Value Iteration</strong>  of  <code>Dynamic_Programming.ipynb</code>. Remember to save your work!</p>
<p>If you&apos;d like to reference the pseudocode while working on the notebook, you are encouraged to open  <a href="https://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf">this sheet</a>  in a new window.</p>
<p>Feel free to check your solution by looking at the corresponding section in  <code>Dynamic_Programming_Solution.ipynb</code>.</p>
<h1 class="mume-header" id="-mini-project-dp-part-6">Mini Project: DP, Part 6</h1>

<p>See the codes <a href="codes/DP_mini_project_part6/Dynamic_Programming_Solution.ipynb">here</a></p>
<h1 class="mume-header" id="-check-your-understanding">Check Your Understanding</h1>

<p>Congratulations! At this point in the lesson, you have written your own implementations of many classical dynamic programming algorithms. This is no easy feat, and you should be proud of all of your hard work!</p>
<p>We encourage you to take your time with this content. Tinker more with the mini project to develop your intuition, and read Chapter 4 (especially 4.1-4.4) of the  <a href="http://go.udacity.com/rl-textbook">textbook</a>  to supplement your understanding.</p>
<p><strong>You are strongly encouraged to take your own notes</strong>. You may find it useful to compare your notes with the next concept, which contains a summary of the main ideas from the lesson.</p>
<p>When you&apos;re ready, answer the question below to check your memory of the terminology.</p>
<h1 class="mume-header" id="-summary">Summary</h1>

<p align="center">
<img src="img/st18.png" alt="drawing" width="750">
</p>
<p align="center">
<img src="img/st19.png" alt="drawing" width="750">
</p>
<p align="center">
<img src="img/st20.png" alt="drawing" width="750">
</p>
<p align="center">
<img src="img/st21.png" alt="drawing" width="750">
</p>
<p align="center">
<img src="img/st22.png" alt="drawing" width="750">
</p>

      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>