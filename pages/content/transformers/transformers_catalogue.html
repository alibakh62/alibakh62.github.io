<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>transformers_catalogue.md</title>
    <link rel="stylesheet" href="https://stackedit.io/style.css" />
    <style>
      /* Style the button that is used to open and close the collapsible content */
      .collapsible {
        background-color: #eee;
        color: #444;
        cursor: pointer;
        padding: 18px;
        width: 100%;
        border: none;
        text-align: left;
        outline: none;
        font-size: 15px;
      }

      /* Add a background color to the button if it is clicked on (add the .active class with JS), and when you move the mouse over it (hover) */
      .active,
      .collapsible:hover {
        background-color: #ccc;
      }

      /* Style the collapsible content. Note: hidden by default */
      .content {
        padding: 0 18px;
        display: none;
        overflow: hidden;
        background-color: #f1f1f1;
      }
    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
    <script>
      hljs.initHighlightingOnLoad();
    </script>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/default.min.css"
    />
  </head>

  <body class="stackedit">
    <div class="stackedit__left">
      <div class="stackedit__toc">
        <ul>
          <li>
            <a
              href="#transformer-models-an-introduction-and-catalog-—-jan.-2023-edition"
              >Transformer models: an introduction and catalog — Jan. 2023
              Edition</a
            >
            <ul>
              <li></li>
              <li>
                <a href="#what-are-transformers">What are Transformers</a>
              </li>
              <li>
                <a href="#encoderdecoder-architecture"
                  >Encoder/Decoder architecture</a
                >
              </li>
              <li><a href="#attention">Attention</a></li>
              <li><a href="#rlhf">RLHF</a></li>
              <li><a href="#diffusion-models">Diffusion Models</a></li>
              <li>
                <a href="#the-transformers-catalog">The Transformers catalog</a>
              </li>
              <li>
                <a href="#pretraining-architecture">Pretraining Architecture</a>
              </li>
              <li><a href="#pretraining-task">Pretraining Task</a></li>
              <li><a href="#application">Application</a></li>
              <li>
                <a href="#chronological-timeline">Chronological Timeline</a>
              </li>
              <li><a href="#catalog-list">Catalog List</a></li>
              <li><a href="#further-reading">Further reading</a></li>
            </ul>
          </li>
        </ul>
      </div>
    </div>
    <div class="stackedit__right">
      <div class="stackedit__html">
        <h1
          id="transformer-models-an-introduction-and-catalog-—-jan.-2023-edition"
        >
          Transformer Models: An Introduction & Catalog 
        </h1>
        <p>
          This article is written by
          <a href="https://amatriain.net/blog/">Xavier Amatriain</a> in January
          2023.
        </p>
        <p><strong>Update 01/16/2023</strong></p>
        <p>
          Six months after my last update, it is clear that the world has been
          taken by storm by Transformers. Everyone is talking about
          <a href="https://amatriain.net/blog/chatGPT">ChatGPT</a>, so I thought
          I needed to add the models that got us there. I had already talked
          about GPTInstruct before, but I added
          <a
            href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#gpt35"
            >GPT3.5</a
          >
          and
          <a
            href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#chatgpt"
            >ChatGPT</a
          >
          as independent models although they don’t add too much to the former.
          I also added a couple of models from
          <a href="https://www.eleuther.ai/">Eleuther.ai</a> and
          <a href="https://www.anthropic.com/">Anthropic</a>, the only two
          startups that seem to be even ready to challenge the
          OpenAI/Facebook/Google supremacy in language models. Because of what
          is happening with ChatGPT, I thought I should add the main competitors
          from the big labs:
          <a
            href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#Sparrow"
            >Sparrow</a
          >
          from Deepmind/Google, and Blenderbot3 from Facebook. Speaking of
          startups though, there has been a lot of talk of
          <a href="https://stability.ai/">Stability.ai</a>, so I felt I needed
          to add a reference to
          <a
            href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#stablediffusion"
            >StableDiffusion</a
          >. Finally, and while not many details are known about
          <a
            href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#alphafold"
            >AlphaFold</a
          >’s architeccture, I thought I should add a reference to it since the
          problem of protein folding is very important, and Deepmind’s
          accomplishment in this regard is huge.
        </p>
        <p>
          Also, there are two concepts that are becoming more and more important
          in the recent success of Transformers: On the one hand,
          <a
            href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#rlhf"
            >RLHF</a
          >
          (Reinforcement Learning with Human Feedback) for language models. On
          the other hand,
          <a
            href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#diffusion"
            >Diffusion models</a
          >. I added a brief section on both these topics.
        </p>
        <p>
          Now that I am including over 50 Transformers I thought I should
          highlight those that for some reason I consider to be noteworthy. I
          hope the others don’t feel bad about it :-) I also felt that very
          often I was searching for Transformer model timelines and none was
          comprehensive enough, so I bit the bullet and added a
          <a
            href="https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/#Timeline"
            >timeline view</a
          >
          to the catalog.
        </p>
        <p>Enjoy! And, as always, human feedback is welcomed.</p>
        <h3 id="why-this-post">Why this post</h3>
        <p>
          I have a terrible memory for names. In the past few years we have seen
          the meteoric appearance of dozens of models of the Transformer family,
          all of which have funny, but not self-explanatory, names. The goal of
          this post is to offer a short and simple catalog and classification of
          the most popular Transformer models. In other words, I needed a
          Transformers cheat-sheet and couldn’t find a good enough one online,
          so I thought I’d write my own. I hope it can be useful to you too.
        </p>
        <h2 id="what-are-transformers">What are Transformers</h2>
        <p>
          Transformers are a class of deep learning models that are defined by
          some architectural traits. They were first introduced in the now
          famous
          <a href="https://arxiv.org/abs/1706.03762"
            >Attention is All you Need</a
          >
          paper by Google researchers in 2017 (the paper has accumulated a
          whooping 38k citations in only 5 years) and associated
          <a
            href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
            >blog post</a
          >.
        </p>
        <p>
          The Transformer architecture is a specific instance of the
          <a
            href="https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/"
            >encoder-decoder models</a
          >
          that had become popular just over the 2–3 years prior. Up until that
          point however, attention was just one of the mechanisms used by these
          models, which were mostly based on LSTM (Long Short Term Memory) and
          other RNN (Recurrent Neural Networks) variations. The key insight of
          the Transformers paper was that, as the title implies, attention could
          be used as the only mechanism to derive dependencies between input and
          output.
        </p>
        <p>
          It is beyond the scope of this blog to go into all the details of the
          Transformer architecture. For that, I will refer you to the original
          paper above or to the wonderful
          <a href="https://jalammar.github.io/illustrated-transformer/"
            >The Illustrated Transformer</a
          >
          post. That being said, we will briefly describe the most important
          aspects since we will be referring to them in the catalog below. Let’s
          start with the basic architectural diagram from the original paper,
          and describe some of the components.
        </p>
        <p><img src="https://amatriain.net/blog/images/02-02.png" alt="" /></p>
        <h2 id="encoderdecoder-architecture">Encoder/Decoder architecture</h2>
        <p>
          A generic encoder/decoder architecture is made up of two models. The
          encoder takes the input and encodes it into a fixed-length vector. The
          decoder takes that vector and decodes it into the output sequence. The
          encoder and decoder are jointly trained to minimize the conditional
          log-likelihood. Once trained the encoder/decoder can generate an
          output given an input sequence or can score a pair of input/output
          sequences.
        </p>
        <p>
          In the case of the original Transformer architecture, both encoder and
          decoder had 6 identical layers. In each of those 6 layers the Encoder
          has two sub layers: a multi-head attention layer, and a simple feed
          forward network. Each sublayer has a residual connection and a layer
          normalization. The output size of the Encoder is 512. The Decoder adds
          a third sublayer, which is another multi-head attention layer over the
          output of the Encoder. Besides, the other multi-head layer in the
          decoder is masked to prevent attention to subsequent positions.
        </p>
        <h2 id="attention">Attention</h2>
        <p>
          It is clear from the description above that the only “exotic” elements
          of the model architecture are the multi-headed attention, but, as
          described above, that is where the whole power of the model lies! So,
          what is attention anyway? An attention function is a mapping between a
          query and a set of key-value pairs to an output. The output is
          computed as a weighted sum of the values, where the weight assigned to
          each value is computed by a compatibility function of the query with
          the corresponding key. Transformers use multi-headed attention, which
          is a parallel computation of a specific attention function called
          scaled dot-product attention. I will refer you again to the
          <a href="https://jalammar.github.io/illustrated-transformer/"
            >The Illustrated Transformer</a
          >
          post for many more details on how the attention mechanism works, but
          will reproduce the diagram from the original paper here so you get the
          main idea
        </p>
        <p><img src="https://amatriain.net/blog/images/02-03.png" alt="" /></p>
        <p>
          There are several advantages of attention layers over recurrent and
          convolutional networks, the two most important being their lower
          computational complexity and their higher connectivity, especially
          useful for learning long-term dependencies in sequences.
        </p>
        <h3 id="what-are-transformers-used-for-and-why-are-they-so-popular">
          What are Transformers used for and why are they so popular
        </h3>
        <p>
          The original transformer was designed for language translation,
          particularly from English to German. But, already the original paper
          showed that the architecture generalized well to other language tasks.
          This particular trend became quickly noticed by the research
          community. Over the next few months most of the leaderboards for any
          language-related ML task became completely dominated by some version
          of the transformer architecture (see for example the well known
          <a href="https://rajpurkar.github.io/SQuAD-explorer/"
            >SQUAD leaderboard</a
          >
          for question answer where all models at the top are ensembles of
          Transformers).
        </p>
        <p>
          One of the key reasons Transformers were able to so quickly take over
          most NLP leaderboards is their ability to quickly adapt to other
          tasks, a.k.a. Transfer learning. Pretrained Transformer models can
          adapt extremely easily and quickly to tasks they have not been trained
          on, and that has huge advantages. As an ML practitioner, you no longer
          need to train a large model on a huge dataset. All you need to do is
          re-use the pretrained model on your task, maybe just slightly adapting
          it with a much smaller data set. A specific technique used to adapt
          pretrained models to a different task is the so-called
          <a href="https://huggingface.co/docs/transformers/training"
            >fine tuning</a
          >.
        </p>
        <p>
          It turns out that the capability of Transformers to adapt to other
          tasks is so great, that, while they were initially developed for
          language related tasks, they quickly became useful for other tasks
          ranging from <a href="https://arxiv.org/abs/2101.01169">vision</a> or
          audio and
          <a href="https://magenta.tensorflow.org/music-transformer">music</a>
          applications all the way to
          <a href="https://arxiv.org/abs/2008.04057">playing chess</a> or
          <a href="https://arxiv.org/abs/2110.03501">doing math</a>!
        </p>
        <p>
          Of course all these applications would have not been possible if it
          wasn’t because of the myriad of tools that made them readily available
          to anyone that could write a few lines of code. Not only were
          Transformers quickly integrated into the main AI frameworks (namely
          <a
            href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
            >Pytorch</a
          >
          and
          <a href="https://www.tensorflow.org/text/tutorials/transformer">TF</a
          >), but they even enabled the creation of an entire company around
          them. <a href="https://huggingface.co/docs">Huggingface</a>, a startup
          that has raised over $60M to this day, is almost entirely built around
          the idea of commercializing their open source
          <a href="https://github.com/huggingface/transformers"
            >Transformers library</a
          >.
        </p>
        <p>
          Last but not least, I would be remiss if I did not mention the impact
          of <a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a> on the
          popularization of Transformers. GPT-3 is a Transformer model
          introduced by OpenAI in May 2020 as a follow up to their earlier GPT
          and GPT-2. The company made a big splash by introducing the model in a
          <a href="https://arxiv.org/abs/2005.14165">preprint</a> in which they
          claimed that the model was so powerful that they were not in a
          position to release it to the world. Since then, the model has not
          only been released, but also commercialized through a very large
          <a
            href="https://openai.com/blog/openai-licenses-gpt-3-technology-to-microsoft/"
            >partnership</a
          >
          between OpenAI and Microsoft. GPT-3 powers over
          <a href="https://openai.com/blog/gpt-3-apps/"
            >300 different applications</a
          >, and is the foundation for OpenAI’s commercial strategy (which is a
          lot to say for a company that has received over $1B in funding).
        </p>
        <h2 id="rlhf">RLHF</h2>
        <p>
          Reinforcement Learning from Human Feedback (or Preferences) aka RLHF
          (or RLHP) has become a huge addition to the AI toolkit as of lately.
          The concept was introduced already in 2017 in the paper
          <a href="https://arxiv.org/abs/1706.03741"
            >“Deep reinforcement learning from human preferences”</a
          >. More recently though, it has been applied to ChatGPT and similar
          dialog agents like BlenderBot3 or Sparrow. The idea is pretty simple
          though: Once a language model is pretrained, we can generate different
          responses to a dialog and have Humans rank the results. We can use
          those ranking (aka preferences or feedback) to train a reward, in the
          reinforcement learning context. You can read much more in these two
          wonderful posts by
          <a href="https://huggingface.co/blog/rlhf">Huggingface</a> or
          <a
            href="https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx"
            >Weights and Bias</a
          >.
        </p>
        <p><img src="https://amatriain.net/blog/images/rlhf.png" alt="" /></p>
        <p>
          From HuggingFace’s RLHF
          <a href="https://huggingface.co/blog/rlhf">blog post</a>.
        </p>
        <h2 id="diffusion-models">Diffusion Models</h2>
        <p>
          Diffusion models have become the new SOTA in image generation, clearly
          pushing aside the previous approaches such as GANs (Generative
          Adversarial Networks). What are diffusion models? They are a class of
          latent variable models trained variational inference. What this means
          in practice is that we train a deep neural network to denoise images
          blurred with some sort of noise function. Networks that are trained
          this way are in fact learning the latent space of what those images
          represent.
        </p>
        <p>
          <img src="https://amatriain.net/blog/images/diffusion.png" alt="" />
        </p>
        <p>
          From
          <a href="https://arxiv.org/abs/2209.00796"
            >“Diffusion Models: A Comprehensive Survey of Methods and
            Applications”</a
          >
        </p>
        <p>
          Diffusion models have relation with other generative models like the
          famous
          <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network"
            >Generative Adversarial Networks (GAN)</a
          >, which they have mostly replaced in many applications and,
          particularly with (denoising) Autoencoders. Some
          <a href="https://benanne.github.io/2022/01/31/diffusion.html"
            >authors</a
          >
          will go as far as saying that Diffusion models are just a specific
          instance of autoencoders. However, they also admit that the small
          differences do transform their application, from the latent
          representation of autoconders to the pure generative nature of
          Diffusion models.
        </p>
        <h2 id="the-transformers-catalog">The Transformers catalog</h2>
        <p>
          So hopefully by now you understand what Transformer models are, and
          why they are so popular and impactful. In this section I will
          introduce a catalog of the most important Transformer models that have
          been developed to this day. I will categorize each model according to
          the following properties: Pretraining Architecture, Pretraining Task,
          Compression, Application, Year, and Number of Parameters. Let’s
          briefly define each of those:
        </p>
        <h2 id="pretraining-architecture">Pretraining Architecture</h2>
        <p>
          We described the Transformer architecture as being made up of an
          Encoder and a Decoder, and that is true for the original Transformer.
          However, since then, different advances have been made that have
          revealed that in some cases it is beneficial to use only the encoder,
          only the decoder, or both.
        </p>
        <p><strong>Encoder Pretraining</strong></p>
        <p>
          These models, which are also called bi-directional or auto-encoding,
          only use the encoder during pretraining, which is usually accomplished
          by masking words in the input sentence and training the model to
          reconstruct. At each stage during pretraining, attention layers can
          access all the input words. This family of models are most useful for
          tasks that require understanding complete sentences such as sentence
          classification or extractive question answering.
        </p>
        <p><strong>Decoder Pretraining</strong></p>
        <p>
          Decoder models, often called auto-regressive, use only the decoder
          during a pretraining that is usually designed so the model is forced
          to predict the next word. The attention layers can only access the
          words positioned before a given word in the sentence. They are best
          suited for tasks involving text generation.
        </p>
        <p><strong>Transformer (Encoder-Decoder) Pretraining</strong></p>
        <p>
          Encoder-decoder models, also called sequence-to-sequence, use both
          parts of the Transformer architecture. Attention layers of the encoder
          can access all the words in the input, while those of the decoder can
          only access the words positioned before a given word in the input. The
          pretraining can be done using the objectives of encoder or decoder
          models, but usually involves something a bit more complex. These
          models are best suited for tasks revolving around generating new
          sentences depending on a given input, such as summarization,
          translation, or generative question answering.
        </p>
        <h2 id="pretraining-task">Pretraining Task</h2>
        <p>
          When training a model we need to define a task for the model to learn
          on. Some of the typical tasks, such as predicting the next word or
          learning to reconstruct masked words were already mentioned above. “<a
            href="https://arxiv.org/abs/2003.08271"
            >Pre-trained Models for Natural Language Processing: A Survey</a
          >” includes a pretty comprehensive taxonomy of pretraining tasks, all
          of which can be considered self-supervised:
        </p>
        <ol>
          <li>
            <strong>Language Modeling (LM):</strong> Predict next token (in the
            case of unidirectional LM) or previous and next token (in the case
            of bidirectional LM)
          </li>
          <li>
            <strong>Masked Language Modeling (MLM):</strong> mask out some
            tokens from the input sentences and then trains the model to predict
            the masked tokens by the rest of the tokens
          </li>
          <li>
            <strong>Permuted Language Modeling (PLM):</strong> same as LM but on
            a random permutation of input sequences. A permutation is randomly
            sampled from all possible permutations. Then some of the tokens are
            chosen as the target, and the model is trained to predict these
            targets.
          </li>
          <li>
            <strong>Denoising Autoencoder (DAE):</strong> take a partially
            corrupted input (e.g. Randomly sampling tokens from the input and
            replacing them with [MASK] elements. randomly deleting tokens from
            the input, or shuffling sentences in random order) and aim to
            recover the original undistorted input.
          </li>
          <li>
            <strong>Contrastive Learning (CTL):</strong> A score function for
            text pairs is learned by assuming some observed pairs of text that
            are more semantically similar than randomly sampled text. It
            includes: <strong><em>Deep InfoMax (DIM):</em></strong>
            <em
              >maximize mutual information between an image representation and
              local regions of the image;</em
            >
            <strong><em>Replaced Token Detection (RTD):</em></strong>
            <em>predict whether a token is replaced given its surroundings;</em>
            <strong><em>Next Sentence Prediction (NSP):</em></strong>
            <em
              >train the model to distinguish whether two input sentences are
              continuous segments from the training corpus; and</em
            >
            <strong><em>Sentence Order Prediction (SOP):</em></strong>
            <em
              >Similar to NSP, but uses two consecutive segments as positive
              examples, and the same segments but with their order swapped as
              negative examples</em
            >
          </li>
        </ol>
        <h2 id="application">Application</h2>
        <p>
          Here we will note what are the main practical applications of the
          Transformer model. Most of these applications will be in the language
          domain (e.g. question answering, sentiment analysis, or entity
          recognition). However, as mentioned before, some Transformer models
          have also found applications well beyond NLP and are also included in
          the catalog.
        </p>
        <h3 id="catalog-table">Catalog table</h3>
        <p>
          <strong>Note:</strong> For all the models available in Huggingface, I
          decided to directly link to the page in the documentation since they
          do a fantastic job of offering a consistent format and links to
          everything else you might need, including the original papers. Only a
          few of the models (e.g. GPT3) are not included in Huggingface.
        </p>
        <p><img src="https://amatriain.net/blog/images/02-04.png" alt="" /></p>
        <p>
          You can access the original table
          <a
            href="https://docs.google.com/spreadsheets/d/1ltyrAB6BL29cOv2fSpNQnnq2vbX8UrHl47d7FkIf6t4/edit#gid=0"
            >here</a
          >
          for easier browsing across the different model features. If you prefer
          to read the full list see below.
        </p>
        <h3 id="family-tree">Family Tree</h3>
        <p>
          The diagram below is just a simple view that should highlight the
          different families of transformers and how they relate to each other.
        </p>
        <p><img src="https://amatriain.net/blog/images/02-05.png" alt="" /></p>
        <h2 id="chronological-timeline">Chronological Timeline</h2>
        <p>
          Another interesting perspective of the catalog is to see it as a
          chronological timeline. Here you will find all the transformers in the
          catalog sorted by their date of publication. In this first
          visualization, the Y-axis is only used to cluster transformers of
          related heritage/family.
        </p>
        <p><img src="https://amatriain.net/blog/images/02-06.png" alt="" /></p>
        <p>
          In this next visualization, the Y-axis represents model size in
          millions of parameters. You won’t be able to see all the models in the
          catalog since many fall right on the same time and size, so please
          refer to the previous image for that.
        </p>
        <p><img src="https://amatriain.net/blog/images/02-09.png" alt="" /></p>
        <h2 id="catalog-list">Catalog List</h2>
        <p>
          Finally, here is a list view that might be easier to follow along in
          some cases:
        </p>

        <iframe
          src="https://docs.google.com/spreadsheets/d/e/2PACX-1vRpy_3cDPOBlFFEKbijyLuBNkZ3-28-hC8-IqW8bgRwGmk8sSt0iMTa7e3AZ3MPVh4ykD0GK9uxMilh/pubhtml?gid=1095853200&amp;single=true&amp;widget=true&amp;headers=false"
          width="100%"
          height="800"
        ></iframe>

        <h2 id="further-reading">Further reading</h2>
        <p>
          Most of the following references have already been mentioned in the
          post. However, it is worth listing them here in case you need more
          details:
        </p>
        <ul>
          <li>
            The Huggingface Transformers
            <a href="https://huggingface.co/course/chapter1/1?fw=pt"
              >documentation</a
            >
            and course is extremely good and comprehensive. I have used myself
            in this post, and I can’t recommend enough as a natural follow up to
            what you will find here.
          </li>
          <li>
            <a href="https://arxiv.org/abs/2106.04554"
              >A survey of transformers</a
            >
            (Lin et al. 2021) includes a 40 page long survey wit over 170
            references and a full blown taxonomy.
          </li>
          <li>
            <a href="https://arxiv.org/pdf/2003.08271.pdf"
              >Pre-trained Models for Natural Language Processing: A Survey</a
            >
            is also a very comprehensive survey that includes many of the
            pretrained models with a particular focus on NLP
          </li>
        </ul>
        <p><img src="https://amatriain.net/blog/images/02-07.png" alt="" /></p>
        <ul>
          <li>
            <a href="https://arxiv.org/abs/2003.08271"
              >Pre-trained Models for Natural Language Processing: A Survey</a
            >
            (Quiu et al. 2021) is another 30+ pages long survey that focuses on
            pretrained models for NLP.
          </li>
        </ul>
        <p><img src="https://amatriain.net/blog/images/02-08.png" alt="" /></p>
      </div>
    </div>
    <script>
      var coll = document.getElementsByClassName("collapsible");
      var i;

      for (i = 0; i < coll.length; i++) {
        coll[i].addEventListener("click", function () {
          this.classList.toggle("active");
          var content = this.nextElementSibling;
          if (content.style.display === "block") {
            content.style.display = "none";
          } else {
            content.style.display = "block";
          }
        });
      }
    </script>
    <script src="https://gist.github.com/username/a39a422ebdff6e732753b90573100b16.js"></script>
  </body>
</html>
